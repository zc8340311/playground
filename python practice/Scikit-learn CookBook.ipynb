{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##Getting sample data from the example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boston House Prices dataset\n",
      "\n",
      "Notes\n",
      "------\n",
      "Data Set Characteristics:  \n",
      "\n",
      "    :Number of Instances: 506 \n",
      "\n",
      "    :Number of Attributes: 13 numeric/categorical predictive\n",
      "    \n",
      "    :Median Value (attribute 14) is usually the target\n",
      "\n",
      "    :Attribute Information (in order):\n",
      "        - CRIM     per capita crime rate by town\n",
      "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "        - INDUS    proportion of non-retail business acres per town\n",
      "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "        - NOX      nitric oxides concentration (parts per 10 million)\n",
      "        - RM       average number of rooms per dwelling\n",
      "        - AGE      proportion of owner-occupied units built prior to 1940\n",
      "        - DIS      weighted distances to five Boston employment centres\n",
      "        - RAD      index of accessibility to radial highways\n",
      "        - TAX      full-value property-tax rate per $10,000\n",
      "        - PTRATIO  pupil-teacher ratio by town\n",
      "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
      "        - LSTAT    % lower status of the population\n",
      "        - MEDV     Median value of owner-occupied homes in $1000's\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
      "\n",
      "This is a copy of UCI ML housing dataset.\n",
      "http://archive.ics.uci.edu/ml/datasets/Housing\n",
      "\n",
      "\n",
      "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
      "\n",
      "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
      "prices and the demand for clean air', J. Environ. Economics & Management,\n",
      "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
      "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
      "pages 244-261 of the latter.\n",
      "\n",
      "The Boston house-price data has been used in many machine learning papers that address regression\n",
      "problems.   \n",
      "     \n",
      "**References**\n",
      "\n",
      "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
      "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
      "   - many more! (see http://archive.ics.uci.edu/ml/datasets/Housing)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "boston=datasets.load_boston()\n",
    "print boston.DESCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading Cal. housing from http://lib.stat.cmu.edu/modules.php?op=modload&name=Downloads&file=index&req=getit&lid=83 to C:\\Users\\zc\\scikit_learn_data\n",
      "California housing dataset.\n",
      "\n",
      "The original database is available from StatLib\n",
      "\n",
      "    http://lib.stat.cmu.edu/\n",
      "\n",
      "The data contains 20,640 observations on 9 variables.\n",
      "\n",
      "This dataset contains the average house value as target variable\n",
      "and the following input variables (features): average income,\n",
      "housing average age, average rooms, average bedrooms, population,\n",
      "average occupation, latitude, and longitude in that order.\n",
      "\n",
      "References\n",
      "----------\n",
      "\n",
      "Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\n",
      "Statistics and Probability Letters, 33 (1997) 291-297.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "housing=datasets.fetch_california_housing()\n",
    "print housing.DESCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.datasets as d\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d.make_*?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d.make_regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data=d.make_regression(1000,10,5,2,1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##scaling data to the standard normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_samples=1000\n",
    "n_features=10\n",
    "X = np.random.randn(n_samples, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00922185  0.04986512  0.02615653]\n",
      "[ 0.98663811  1.02358573  0.98750061]\n"
     ]
    }
   ],
   "source": [
    "print X[:,:3].mean(axis=0)\n",
    "print X[:,:3].std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  1.,  1.])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2=preprocessing.scale(X[:,:3])\n",
    "X2.mean(axis=0)\n",
    "X2.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minmax_scaler=preprocessing.MinMaxScaler()\n",
    "minmax_scaler.fit(X[:,:3])\n",
    "x3=minmax_scaler.transform(X[:,:3])\n",
    "x3.max(axis=0)\n",
    "x3.min(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.14,  3.14,  3.14])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##another scaler\n",
    "other_range=preprocessing.MinMaxScaler(feature_range=(-3.14,3.14))\n",
    "other_range.fit(X[:,:3])\n",
    "x4=other_range.transform(X[:,:3])\n",
    "x4.max(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##normalization scale each sample to have a length of 1\n",
    "normalized_x=preprocessing.normalize(X[:,:3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##creating bianry feature through thresholding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "boston=datasets.load_boston()\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  0.  1.  1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  1.  1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.\n",
      "   0.  1.  1.  1.  1.  0.  0.  0.  0.  1.  1.  1.  0.  0.  0.  0.  1.  0.\n",
      "   1.  1.  1.  0.  0.  0.  0.  0.  1.  1.  1.  1.  1.  1.  0.  0.  1.  1.\n",
      "   1.  0.  1.  1.  0.  1.  0.  1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  1.  1.  1.\n",
      "   1.  1.  1.  1.  1.  1.  1.  0.  0.  0.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "   1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "   1.  1.  1.  1.  1.  1.  1.  1.  1.  0.  1.  0.  0.  0.  0.  1.  1.  1.\n",
      "   1.  1.  0.  1.  1.  0.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "   1.  1.  1.  1.  1.  1.  0.  0.  0.  1.  0.  0.  1.  0.  1.  1.  1.  1.\n",
      "   1.  1.  0.  0.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  0.\n",
      "   0.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  0.  0.  1.\n",
      "   0.  1.  1.  1.  1.  1.  0.  1.  1.  0.  0.  1.  1.  0.  1.  1.  1.  1.\n",
      "   1.  1.  1.  0.  0.  0.  0.  0.  1.  0.  0.  0.  1.  0.  1.  1.  0.  0.\n",
      "   1.  1.  1.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  1.  1.  0.  0.  1.  1.  1.  1.  1.  0.  1.  0.  0.  0.  0.  1.  1.\n",
      "   1.  0.  0.  0.  0.  1.  0.  1.  1.  1.  1.  1.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  1.  1.  1.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "new_target=preprocessing.binarize(boston.target,threshold=boston.target.mean())\n",
    "\n",
    "bin=preprocessing.Binarizer(boston.target.mean())\n",
    "new_target=bin.fit_transform(boston.target)\n",
    "print new_target[0:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##working with categoritcal variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris=datasets.load_iris()\n",
    "#print iris.DESCR\n",
    "X=iris.data\n",
    "y=iris.target\n",
    "import numpy as np\n",
    "d=np.column_stack((X,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "text_encoder=preprocessing.OneHotEncoder()\n",
    "text_encoder.fit_transform(d[:,-1:]).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##DictVectorizer\n",
    "another option is to use dictVectorizer.this can be used to directly convert strings to features:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  0.,  1.]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "dv=DictVectorizer()\n",
    "my_dict=[{'species':iris.target_names[i]} for i in y]\n",
    "index=[1,50,120]\n",
    "dv.fit_transform(my_dict).toarray()[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Binarizing label features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets as d\n",
    "iris=d.load_iris()\n",
    "target=iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150L, 3L)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "label_binarizer=LabelBinarizer()\n",
    "new_target=label_binarizer.fit_transform(target)\n",
    "new_target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###more on binarize\n",
    "0 and 1 are ont only way to represent positive instance and negtiveinstance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3, -3, -3],\n",
       "       [ 3, -3, -3],\n",
       "       [ 3, -3, -3],\n",
       "       [ 3, -3, -3],\n",
       "       [ 3, -3, -3],\n",
       "       [ 3, -3, -3],\n",
       "       [ 3, -3, -3],\n",
       "       [ 3, -3, -3],\n",
       "       [ 3, -3, -3],\n",
       "       [ 3, -3, -3],\n",
       "       [ 3, -3, -3],\n",
       "       [ 3, -3, -3],\n",
       "       [ 3, -3, -3],\n",
       "       [ 3, -3, -3],\n",
       "       [ 3, -3, -3],\n",
       "       [ 3, -3, -3],\n",
       "       [ 3, -3, -3],\n",
       "       [ 3, -3, -3],\n",
       "       [ 3, -3, -3],\n",
       "       [ 3, -3, -3],\n",
       "       [ 3, -3, -3],\n",
       "       [ 3, -3, -3],\n",
       "       [ 3, -3, -3],\n",
       "       [ 3, -3, -3],\n",
       "       [ 3, -3, -3],\n",
       "       [ 3, -3, -3],\n",
       "       [ 3, -3, -3],\n",
       "       [ 3, -3, -3],\n",
       "       [ 3, -3, -3],\n",
       "       [ 3, -3, -3],\n",
       "       [ 3, -3, -3],\n",
       "       [ 3, -3, -3],\n",
       "       [ 3, -3, -3],\n",
       "       [ 3, -3, -3],\n",
       "       [ 3, -3, -3],\n",
       "       [ 3, -3, -3],\n",
       "       [ 3, -3, -3],\n",
       "       [ 3, -3, -3],\n",
       "       [ 3, -3, -3],\n",
       "       [ 3, -3, -3],\n",
       "       [ 3, -3, -3],\n",
       "       [ 3, -3, -3],\n",
       "       [ 3, -3, -3],\n",
       "       [ 3, -3, -3],\n",
       "       [ 3, -3, -3],\n",
       "       [ 3, -3, -3],\n",
       "       [ 3, -3, -3],\n",
       "       [ 3, -3, -3],\n",
       "       [ 3, -3, -3],\n",
       "       [ 3, -3, -3],\n",
       "       [-3,  3, -3],\n",
       "       [-3,  3, -3],\n",
       "       [-3,  3, -3],\n",
       "       [-3,  3, -3],\n",
       "       [-3,  3, -3],\n",
       "       [-3,  3, -3],\n",
       "       [-3,  3, -3],\n",
       "       [-3,  3, -3],\n",
       "       [-3,  3, -3],\n",
       "       [-3,  3, -3],\n",
       "       [-3,  3, -3],\n",
       "       [-3,  3, -3],\n",
       "       [-3,  3, -3],\n",
       "       [-3,  3, -3],\n",
       "       [-3,  3, -3],\n",
       "       [-3,  3, -3],\n",
       "       [-3,  3, -3],\n",
       "       [-3,  3, -3],\n",
       "       [-3,  3, -3],\n",
       "       [-3,  3, -3],\n",
       "       [-3,  3, -3],\n",
       "       [-3,  3, -3],\n",
       "       [-3,  3, -3],\n",
       "       [-3,  3, -3],\n",
       "       [-3,  3, -3],\n",
       "       [-3,  3, -3],\n",
       "       [-3,  3, -3],\n",
       "       [-3,  3, -3],\n",
       "       [-3,  3, -3],\n",
       "       [-3,  3, -3],\n",
       "       [-3,  3, -3],\n",
       "       [-3,  3, -3],\n",
       "       [-3,  3, -3],\n",
       "       [-3,  3, -3],\n",
       "       [-3,  3, -3],\n",
       "       [-3,  3, -3],\n",
       "       [-3,  3, -3],\n",
       "       [-3,  3, -3],\n",
       "       [-3,  3, -3],\n",
       "       [-3,  3, -3],\n",
       "       [-3,  3, -3],\n",
       "       [-3,  3, -3],\n",
       "       [-3,  3, -3],\n",
       "       [-3,  3, -3],\n",
       "       [-3,  3, -3],\n",
       "       [-3,  3, -3],\n",
       "       [-3,  3, -3],\n",
       "       [-3,  3, -3],\n",
       "       [-3,  3, -3],\n",
       "       [-3,  3, -3],\n",
       "       [-3, -3,  3],\n",
       "       [-3, -3,  3],\n",
       "       [-3, -3,  3],\n",
       "       [-3, -3,  3],\n",
       "       [-3, -3,  3],\n",
       "       [-3, -3,  3],\n",
       "       [-3, -3,  3],\n",
       "       [-3, -3,  3],\n",
       "       [-3, -3,  3],\n",
       "       [-3, -3,  3],\n",
       "       [-3, -3,  3],\n",
       "       [-3, -3,  3],\n",
       "       [-3, -3,  3],\n",
       "       [-3, -3,  3],\n",
       "       [-3, -3,  3],\n",
       "       [-3, -3,  3],\n",
       "       [-3, -3,  3],\n",
       "       [-3, -3,  3],\n",
       "       [-3, -3,  3],\n",
       "       [-3, -3,  3],\n",
       "       [-3, -3,  3],\n",
       "       [-3, -3,  3],\n",
       "       [-3, -3,  3],\n",
       "       [-3, -3,  3],\n",
       "       [-3, -3,  3],\n",
       "       [-3, -3,  3],\n",
       "       [-3, -3,  3],\n",
       "       [-3, -3,  3],\n",
       "       [-3, -3,  3],\n",
       "       [-3, -3,  3],\n",
       "       [-3, -3,  3],\n",
       "       [-3, -3,  3],\n",
       "       [-3, -3,  3],\n",
       "       [-3, -3,  3],\n",
       "       [-3, -3,  3],\n",
       "       [-3, -3,  3],\n",
       "       [-3, -3,  3],\n",
       "       [-3, -3,  3],\n",
       "       [-3, -3,  3],\n",
       "       [-3, -3,  3],\n",
       "       [-3, -3,  3],\n",
       "       [-3, -3,  3],\n",
       "       [-3, -3,  3],\n",
       "       [-3, -3,  3],\n",
       "       [-3, -3,  3],\n",
       "       [-3, -3,  3],\n",
       "       [-3, -3,  3],\n",
       "       [-3, -3,  3],\n",
       "       [-3, -3,  3],\n",
       "       [-3, -3,  3]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_binarizer=LabelBinarizer(neg_label=-3,pos_label=3)\n",
    "label_binarizer.fit_transform(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##imputing missing values through various strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.1,  3.5,  1.4,  nan],\n",
       "       [ 4.9,  3. ,  nan,  nan],\n",
       "       [ 4.7,  3.2,  1.3,  0.2],\n",
       "       [ nan,  3.1,  1.5,  0.2],\n",
       "       [ 5. ,  3.6,  1.4,  0.2]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "iris=datasets.load_iris()\n",
    "X=iris.data\n",
    "masking_array=np.random.binomial(1,0.25,X.shape).astype(bool)\n",
    "X[masking_array]=np.nan\n",
    "masking_array[:5]\n",
    "X[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.1,  3.5,  1.4,  1.3],\n",
       "       [ 4.9,  3. ,  4.4,  1.3],\n",
       "       [ 4.7,  3.2,  1.3,  0.2],\n",
       "       [ 5.7,  3.1,  1.5,  0.2],\n",
       "       [ 5. ,  3.6,  1.4,  0.2]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "impute=preprocessing.Imputer(strategy='median')\n",
    "iris_X=impute.fit_transform(X)\n",
    "iris_X[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], shape=(5L, 0L), dtype=float64)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "impute2=preprocessing.Imputer(missing_values=1)\n",
    "iris_X2=impute2.fit_transform(X)\n",
    "iris_X2[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Using pipelines for multiple preprocessing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.13471645, -0.23636519,  0.13670053,  0.36800488],\n",
       "       [-0.23636519,  0.87356802, -0.06830902, -0.18464931],\n",
       "       [ 0.13670053, -0.06830902,  0.67469696,  0.02815677],\n",
       "       [ 0.36800488, -0.18464931,  0.02815677,  1.19726575]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "mat=datasets.make_spd_matrix(10)\n",
    "masking_array=np.random.binomial(1,.1,mat.shape).astype(bool)\n",
    "mat[:4,:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('impute', Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)), ('scaler', StandardScaler(copy=True, with_mean=True, with_std=True))])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "impute=preprocessing.Imputer()\n",
    "scaler=preprocessing.StandardScaler()\n",
    "from sklearn import pipeline\n",
    "pipe=pipeline.Pipeline([('impute',impute),('scaler',scaler)])\n",
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.43912342e+00,  -4.18549110e-01,   1.71032643e-03,\n",
       "          1.66914669e-01],\n",
       "       [ -7.63439265e-01,   1.93608863e+00,  -7.57480703e-01,\n",
       "         -6.83277439e-01],\n",
       "       [ -1.64130943e-01,  -6.20309055e-02,   1.99401779e+00,\n",
       "         -3.55900836e-01],\n",
       "       [  2.07445997e-01,  -3.08837853e-01,  -4.00248740e-01,\n",
       "          1.44263296e+00]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##call the fit_transform method on the pipe object\n",
    "##these separate steps are completed in a single step:\n",
    "new_mat=pipe.fit_transform(mat)\n",
    "new_mat[:4,:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Reducing dimensionality with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris=datasets.load_iris()\n",
    "iris_X=iris.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, n_components=None, whiten=False)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import decomposition\n",
    "pca=decomposition.PCA()\n",
    "pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -2.68420713e+00,  -3.26607315e-01,   2.15118370e-02,\n",
       "          1.00615724e-03],\n",
       "       [ -2.71539062e+00,   1.69556848e-01,   2.03521425e-01,\n",
       "          9.96024240e-02],\n",
       "       [ -2.88981954e+00,   1.37345610e-01,  -2.47092410e-02,\n",
       "          1.93045428e-02],\n",
       "       [ -2.74643720e+00,   3.11124316e-01,  -3.76719753e-02,\n",
       "         -7.59552741e-02],\n",
       "       [ -2.72859298e+00,  -3.33924564e-01,  -9.62296998e-02,\n",
       "         -6.31287327e-02]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_pca=pca.fit_transform(iris_X)\n",
    "iris_pca[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.92461621,  0.05301557,  0.01718514,  0.00518309])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150L, 2L)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca=decomposition.PCA(n_components=2)\n",
    "iris_X_prime=pca.fit_transform(iris_X)\n",
    "iris_X_prime.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99481691454980992"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##pca explain at least 98 percent of the variance\n",
    "pca=decomposition.PCA(n_components=.98)\n",
    "iris_X_prime=pca.fit(iris_X)\n",
    "pca.explained_variance_ratio_.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Using factor analysis for decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import FactorAnalysis\n",
    "from sklearn import datasets\n",
    "iris=datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.33125848,  0.55846779],\n",
       "       [-1.33914102, -0.00509715],\n",
       "       [-1.40258715, -0.307983  ],\n",
       "       [-1.29839497, -0.71854288],\n",
       "       [-1.33587575,  0.36533259]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fa=FactorAnalysis(n_components=2)\n",
    "iris_two_dim=fa.fit_transform(iris.data)\n",
    "iris_two_dim[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Kennel PCA for nonlinear dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zc\\AppData\\Local\\Enthought\\Canopy\\User\\lib\\site-packages\\IPython\\kernel\\__main__.py:14: RuntimeWarning: covariance is not positive-semidefinite.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "A1_mean=[1,1]\n",
    "A1_cov=[[2,.99],[1,1]]\n",
    "A1=np.random.multivariate_normal(A1_mean,A1_cov,50)\n",
    "\n",
    "A2_mean=[5,5]\n",
    "A2_cov=[[2,.99],[1,1]]\n",
    "A2=np.random.multivariate_normal(A2_mean,A2_cov,50)\n",
    "\n",
    "A=np.vstack((A1,A2))\n",
    "\n",
    "B_mean=[5,0]\n",
    "B_cov=[[.5,-1],[-.9,.5]]\n",
    "B=np.random.multivariate_normal(B_mean,B_cov,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kpca=decomposition.KernelPCA(kernel='cosine',n_components=1)\n",
    "AB=np.vstack((A,B))\n",
    "AB_transformed=kpca.fit_transform(AB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Using truncated SVD to reduce dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris=load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5.1  3.5  1.4  0.2]\n",
      " [ 4.9  3.   1.4  0.2]\n",
      " [ 4.7  3.2  1.3  0.2]\n",
      " [ 4.6  3.1  1.5  0.2]\n",
      " [ 5.   3.6  1.4  0.2]]\n",
      "[[ 5.91220352 -2.30344211]\n",
      " [ 5.57207573 -1.97383104]\n",
      " [ 5.4464847  -2.09653267]\n",
      " [ 5.43601924 -1.87168085]\n",
      " [ 5.87506555 -2.32934799]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd=TruncatedSVD(2)\n",
    "iris_tran=svd.fit_transform(iris.data)\n",
    "print iris.data[:5]\n",
    "print iris_tran[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [1, 3],\n",
       "       [1, 4]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##using scipy to learn svd process\n",
    "from scipy.linalg import svd\n",
    "import numpy as np\n",
    "D=np.array([[1,2],[1,3],[1,4]])\n",
    "D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3L, 2L) (2L,) (2L, 2L)\n"
     ]
    }
   ],
   "source": [
    "U,S,V=svd(D, full_matrices=False)\n",
    "print U.shape,S.shape,V.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  2.],\n",
       "       [ 1.,  3.],\n",
       "       [ 1.,  4.]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(U.dot(np.diag(S)),V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.20719466, -3.16170819, -4.11622173])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#if we want to simulate the truncation, we will drop \n",
    "#the smallest singular values and the corresponding \n",
    "##column vectors of U. \n",
    "new_S=S[0]\n",
    "new_U=U[:,0]\n",
    "new_U.dot(new_S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Decomposition to classify with DictionaryLearning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DictionaryLearning attempts to take a dataset and transform\n",
    "it into a sparse representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris=load_iris()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  6.34476574,  0.        ],\n",
       "       [ 0.        ,  5.83576461,  0.        ],\n",
       "       [ 0.        ,  6.32038375,  0.        ],\n",
       "       [ 0.        ,  5.89318572,  0.        ],\n",
       "       [ 0.        ,  5.45222715,  0.        ]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import DictionaryLearning\n",
    "dl=DictionaryLearning(3)\n",
    "transformed=dl.fit_transform(iris.data[::2])\n",
    "transformed[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Putting it all together with Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from skelearn.datasets import load_iris\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.1,  3.5,  1.4,  0.2],\n",
       "       [ nan,  3. ,  1.4,  0.2],\n",
       "       [ 4.7,  3.2,  nan,  0.2],\n",
       "       [ 4.6,  nan,  1.5,  0.2],\n",
       "       [ 5. ,  nan,  1.4,  0.2]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris=load_iris()\n",
    "iris_data=iris.data\n",
    "mask=np.random.binomial(1,.25,iris_data.shape).astype(bool)\n",
    "iris_data[mask]=np.nan\n",
    "iris_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import pipeline,preprocessing,decomposition\n",
    "\n",
    "##create the imputer and pca classes\n",
    "pca=decomposition.PCA()\n",
    "imputer=preprocessing.Imputer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.69552892,  0.02925068,  0.24571523,  0.2216217 ],\n",
       "       [-2.46034731, -0.56692907,  0.29427252, -0.3890228 ],\n",
       "       [-0.64943393,  1.192394  ,  0.72911798,  0.14641949],\n",
       "       [-2.70092101,  0.59427419,  0.0493901 , -0.13929987],\n",
       "       [-2.68455342,  0.19465465,  0.11210702, -0.217421  ]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##create a pipe line\n",
    "pipe=pipeline.Pipeline([('imputer',imputer),('pca',pca)])\n",
    "iris_data_transformed=pipe.fit_transform(iris_data)\n",
    "iris_data_transformed[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
