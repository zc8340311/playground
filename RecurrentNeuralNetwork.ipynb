{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.contrib import rnn\n",
    "import itertools as ittls\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 784) (500,)\n"
     ]
    }
   ],
   "source": [
    "folder = r\"/home/czhou2/Documents\"\n",
    "X = np.load('/'.join([folder,'train_x_small.pkl']))\n",
    "y = np.load('/'.join([folder,'train_y_small.pkl']))\n",
    "print X.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "learning_rate = 0.001\n",
    "training_steps = 60000\n",
    "batch_size = 128\n",
    "display_step = 200\n",
    "\n",
    "# Network Parameters\n",
    "num_input = 28 # MNIST data input (img shape: 28*28)\n",
    "timesteps = 28 # timesteps\n",
    "num_hidden = 128 # hidden layer num of features\n",
    "num_classes = 10 # MNIST total classes (0-9 digits)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN(x, weights, biases):\n",
    "\n",
    "    # Prepare data shape to match `rnn` function requirements\n",
    "    # Current data input shape: (batch_size, timesteps, n_input)\n",
    "    # Required shape: 'timesteps' tensors list of shape (batch_size, n_input)\n",
    "\n",
    "    # Unstack to get a list of 'timesteps' tensors of shape (batch_size, n_input)\n",
    "    x = tf.unstack(x, timesteps, 1)\n",
    "\n",
    "    # Define a lstm cell with tensorflow\n",
    "    lstm_cell = rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)\n",
    "\n",
    "    # Get lstm cell output\n",
    "    outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "\n",
    "## generate batch index for loop interation\n",
    "def batch_gen(batch_size,polulation_size=500):\n",
    "    collection = []\n",
    "    for _ in ittls.cycle(xrange(polulation_size)):\n",
    "        collection.append(_)\n",
    "        if len(collection) >= batch_size:\n",
    "            yield collection\n",
    "            collection = []\n",
    "\n",
    "def mnist_encoder(y):\n",
    "    encoded_y = np.zeros((len(y),10),dtype=np.float32)\n",
    "    for index,label in enumerate(y):\n",
    "        encoded_y[index,label] = 1.0\n",
    "    return encoded_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Minibatch Loss= 3.0182, Training Accuracy= 0.070\n",
      "Step 200, Minibatch Loss= 2.0679, Training Accuracy= 0.289\n",
      "Step 400, Minibatch Loss= 1.8519, Training Accuracy= 0.445\n",
      "Step 600, Minibatch Loss= 1.6542, Training Accuracy= 0.539\n",
      "Step 800, Minibatch Loss= 1.4743, Training Accuracy= 0.586\n",
      "Step 1000, Minibatch Loss= 1.3991, Training Accuracy= 0.602\n",
      "Step 1200, Minibatch Loss= 1.3153, Training Accuracy= 0.617\n",
      "Step 1400, Minibatch Loss= 1.2004, Training Accuracy= 0.672\n",
      "Step 1600, Minibatch Loss= 1.0698, Training Accuracy= 0.711\n",
      "Step 1800, Minibatch Loss= 0.8985, Training Accuracy= 0.773\n",
      "Step 2000, Minibatch Loss= 1.0192, Training Accuracy= 0.727\n",
      "Step 2200, Minibatch Loss= 0.9730, Training Accuracy= 0.719\n",
      "Step 2400, Minibatch Loss= 0.9167, Training Accuracy= 0.797\n",
      "Step 2600, Minibatch Loss= 0.8385, Training Accuracy= 0.781\n",
      "Step 2800, Minibatch Loss= 0.6746, Training Accuracy= 0.828\n",
      "Step 3000, Minibatch Loss= 0.8304, Training Accuracy= 0.781\n",
      "Step 3200, Minibatch Loss= 0.7758, Training Accuracy= 0.758\n",
      "Step 3400, Minibatch Loss= 0.7683, Training Accuracy= 0.812\n",
      "Step 3600, Minibatch Loss= 0.7050, Training Accuracy= 0.797\n",
      "Step 3800, Minibatch Loss= 0.5409, Training Accuracy= 0.875\n",
      "Step 4000, Minibatch Loss= 0.6998, Training Accuracy= 0.789\n",
      "Step 4200, Minibatch Loss= 0.6454, Training Accuracy= 0.797\n",
      "Step 4400, Minibatch Loss= 0.6810, Training Accuracy= 0.812\n",
      "Step 4600, Minibatch Loss= 0.6135, Training Accuracy= 0.820\n",
      "Step 4800, Minibatch Loss= 0.4507, Training Accuracy= 0.906\n",
      "Step 5000, Minibatch Loss= 0.6000, Training Accuracy= 0.820\n",
      "Step 5200, Minibatch Loss= 0.5534, Training Accuracy= 0.828\n",
      "Step 5400, Minibatch Loss= 0.6115, Training Accuracy= 0.828\n",
      "Step 5600, Minibatch Loss= 0.5367, Training Accuracy= 0.867\n",
      "Step 5800, Minibatch Loss= 0.3808, Training Accuracy= 0.914\n",
      "Step 6000, Minibatch Loss= 0.5178, Training Accuracy= 0.859\n",
      "Step 6200, Minibatch Loss= 0.4803, Training Accuracy= 0.852\n",
      "Step 6400, Minibatch Loss= 0.5448, Training Accuracy= 0.852\n",
      "Step 6600, Minibatch Loss= 0.4642, Training Accuracy= 0.883\n",
      "Step 6800, Minibatch Loss= 0.3224, Training Accuracy= 0.906\n",
      "Step 7000, Minibatch Loss= 0.4461, Training Accuracy= 0.867\n",
      "Step 7200, Minibatch Loss= 0.4167, Training Accuracy= 0.859\n",
      "Step 7400, Minibatch Loss= 0.4773, Training Accuracy= 0.867\n",
      "Step 7600, Minibatch Loss= 0.3937, Training Accuracy= 0.891\n",
      "Step 7800, Minibatch Loss= 0.2725, Training Accuracy= 0.914\n",
      "Step 8000, Minibatch Loss= 0.3809, Training Accuracy= 0.891\n",
      "Step 8200, Minibatch Loss= 0.3584, Training Accuracy= 0.875\n",
      "Step 8400, Minibatch Loss= 0.4094, Training Accuracy= 0.883\n",
      "Step 8600, Minibatch Loss= 0.3258, Training Accuracy= 0.906\n",
      "Step 8800, Minibatch Loss= 0.2300, Training Accuracy= 0.914\n",
      "Step 9000, Minibatch Loss= 0.3206, Training Accuracy= 0.922\n",
      "Step 9200, Minibatch Loss= 0.3041, Training Accuracy= 0.930\n",
      "Step 9400, Minibatch Loss= 0.3421, Training Accuracy= 0.898\n",
      "Step 9600, Minibatch Loss= 0.2625, Training Accuracy= 0.938\n",
      "Step 9800, Minibatch Loss= 0.1935, Training Accuracy= 0.938\n",
      "Step 10000, Minibatch Loss= 0.2656, Training Accuracy= 0.938\n",
      "Step 10200, Minibatch Loss= 0.2540, Training Accuracy= 0.945\n",
      "Step 10400, Minibatch Loss= 0.2778, Training Accuracy= 0.922\n",
      "Step 10600, Minibatch Loss= 0.2067, Training Accuracy= 0.953\n",
      "Step 10800, Minibatch Loss= 0.1608, Training Accuracy= 0.961\n",
      "Step 11000, Minibatch Loss= 0.2171, Training Accuracy= 0.953\n",
      "Step 11200, Minibatch Loss= 0.2083, Training Accuracy= 0.953\n",
      "Step 11400, Minibatch Loss= 0.2207, Training Accuracy= 0.938\n",
      "Step 11600, Minibatch Loss= 0.1610, Training Accuracy= 0.969\n",
      "Step 11800, Minibatch Loss= 0.1306, Training Accuracy= 0.984\n",
      "Step 12000, Minibatch Loss= 0.1744, Training Accuracy= 0.961\n",
      "Step 12200, Minibatch Loss= 0.1669, Training Accuracy= 0.961\n",
      "Step 12400, Minibatch Loss= 0.1723, Training Accuracy= 0.953\n",
      "Step 12600, Minibatch Loss= 0.1248, Training Accuracy= 0.977\n",
      "Step 12800, Minibatch Loss= 0.1031, Training Accuracy= 0.992\n",
      "Step 13000, Minibatch Loss= 0.1374, Training Accuracy= 0.977\n",
      "Step 13200, Minibatch Loss= 0.1300, Training Accuracy= 0.969\n",
      "Step 13400, Minibatch Loss= 0.1313, Training Accuracy= 0.969\n",
      "Step 13600, Minibatch Loss= 0.0955, Training Accuracy= 0.984\n",
      "Step 13800, Minibatch Loss= 0.0795, Training Accuracy= 1.000\n",
      "Step 14000, Minibatch Loss= 0.1065, Training Accuracy= 0.977\n",
      "Step 14200, Minibatch Loss= 0.0977, Training Accuracy= 0.977\n",
      "Step 14400, Minibatch Loss= 0.0967, Training Accuracy= 0.977\n",
      "Step 14600, Minibatch Loss= 0.0721, Training Accuracy= 0.992\n",
      "Step 14800, Minibatch Loss= 0.0605, Training Accuracy= 1.000\n",
      "Step 15000, Minibatch Loss= 0.0815, Training Accuracy= 0.984\n",
      "Step 15200, Minibatch Loss= 0.0720, Training Accuracy= 0.992\n",
      "Step 15400, Minibatch Loss= 0.0695, Training Accuracy= 0.992\n",
      "Step 15600, Minibatch Loss= 0.0544, Training Accuracy= 1.000\n",
      "Step 15800, Minibatch Loss= 0.0459, Training Accuracy= 1.000\n",
      "Step 16000, Minibatch Loss= 0.0616, Training Accuracy= 0.992\n",
      "Step 16200, Minibatch Loss= 0.0536, Training Accuracy= 1.000\n",
      "Step 16400, Minibatch Loss= 0.0504, Training Accuracy= 1.000\n",
      "Step 16600, Minibatch Loss= 0.0417, Training Accuracy= 1.000\n",
      "Step 16800, Minibatch Loss= 0.0351, Training Accuracy= 1.000\n",
      "Step 17000, Minibatch Loss= 0.0462, Training Accuracy= 0.992\n",
      "Step 17200, Minibatch Loss= 0.0410, Training Accuracy= 1.000\n",
      "Step 17400, Minibatch Loss= 0.0375, Training Accuracy= 1.000\n",
      "Step 17600, Minibatch Loss= 0.0326, Training Accuracy= 1.000\n",
      "Step 17800, Minibatch Loss= 0.0271, Training Accuracy= 1.000\n",
      "Step 18000, Minibatch Loss= 0.0349, Training Accuracy= 0.992\n",
      "Step 18200, Minibatch Loss= 0.0321, Training Accuracy= 1.000\n",
      "Step 18400, Minibatch Loss= 0.0287, Training Accuracy= 1.000\n",
      "Step 18600, Minibatch Loss= 0.0260, Training Accuracy= 1.000\n",
      "Step 18800, Minibatch Loss= 0.0212, Training Accuracy= 1.000\n",
      "Step 19000, Minibatch Loss= 0.0268, Training Accuracy= 1.000\n",
      "Step 19200, Minibatch Loss= 0.0256, Training Accuracy= 1.000\n",
      "Step 19400, Minibatch Loss= 0.0226, Training Accuracy= 1.000\n",
      "Step 19600, Minibatch Loss= 0.0211, Training Accuracy= 1.000\n",
      "Step 19800, Minibatch Loss= 0.0169, Training Accuracy= 1.000\n",
      "Step 20000, Minibatch Loss= 0.0212, Training Accuracy= 1.000\n",
      "Step 20200, Minibatch Loss= 0.0208, Training Accuracy= 1.000\n",
      "Step 20400, Minibatch Loss= 0.0182, Training Accuracy= 1.000\n",
      "Step 20600, Minibatch Loss= 0.0174, Training Accuracy= 1.000\n",
      "Step 20800, Minibatch Loss= 0.0138, Training Accuracy= 1.000\n",
      "Step 21000, Minibatch Loss= 0.0171, Training Accuracy= 1.000\n",
      "Step 21200, Minibatch Loss= 0.0171, Training Accuracy= 1.000\n",
      "Step 21400, Minibatch Loss= 0.0150, Training Accuracy= 1.000\n",
      "Step 21600, Minibatch Loss= 0.0147, Training Accuracy= 1.000\n",
      "Step 21800, Minibatch Loss= 0.0114, Training Accuracy= 1.000\n",
      "Step 22000, Minibatch Loss= 0.0142, Training Accuracy= 1.000\n",
      "Step 22200, Minibatch Loss= 0.0144, Training Accuracy= 1.000\n",
      "Step 22400, Minibatch Loss= 0.0126, Training Accuracy= 1.000\n",
      "Step 22600, Minibatch Loss= 0.0126, Training Accuracy= 1.000\n",
      "Step 22800, Minibatch Loss= 0.0096, Training Accuracy= 1.000\n",
      "Step 23000, Minibatch Loss= 0.0119, Training Accuracy= 1.000\n",
      "Step 23200, Minibatch Loss= 0.0123, Training Accuracy= 1.000\n",
      "Step 23400, Minibatch Loss= 0.0108, Training Accuracy= 1.000\n",
      "Step 23600, Minibatch Loss= 0.0109, Training Accuracy= 1.000\n",
      "Step 23800, Minibatch Loss= 0.0082, Training Accuracy= 1.000\n",
      "Step 24000, Minibatch Loss= 0.0102, Training Accuracy= 1.000\n",
      "Step 24200, Minibatch Loss= 0.0106, Training Accuracy= 1.000\n",
      "Step 24400, Minibatch Loss= 0.0094, Training Accuracy= 1.000\n",
      "Step 24600, Minibatch Loss= 0.0096, Training Accuracy= 1.000\n",
      "Step 24800, Minibatch Loss= 0.0071, Training Accuracy= 1.000\n",
      "Step 25000, Minibatch Loss= 0.0088, Training Accuracy= 1.000\n",
      "Step 25200, Minibatch Loss= 0.0092, Training Accuracy= 1.000\n",
      "Step 25400, Minibatch Loss= 0.0082, Training Accuracy= 1.000\n",
      "Step 25600, Minibatch Loss= 0.0085, Training Accuracy= 1.000\n",
      "Step 25800, Minibatch Loss= 0.0062, Training Accuracy= 1.000\n",
      "Step 26000, Minibatch Loss= 0.0077, Training Accuracy= 1.000\n",
      "Step 26200, Minibatch Loss= 0.0082, Training Accuracy= 1.000\n",
      "Step 26400, Minibatch Loss= 0.0073, Training Accuracy= 1.000\n",
      "Step 26600, Minibatch Loss= 0.0076, Training Accuracy= 1.000\n",
      "Step 26800, Minibatch Loss= 0.0055, Training Accuracy= 1.000\n",
      "Step 27000, Minibatch Loss= 0.0069, Training Accuracy= 1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 27200, Minibatch Loss= 0.0073, Training Accuracy= 1.000\n",
      "Step 27400, Minibatch Loss= 0.0065, Training Accuracy= 1.000\n",
      "Step 27600, Minibatch Loss= 0.0068, Training Accuracy= 1.000\n",
      "Step 27800, Minibatch Loss= 0.0049, Training Accuracy= 1.000\n",
      "Step 28000, Minibatch Loss= 0.0061, Training Accuracy= 1.000\n",
      "Step 28200, Minibatch Loss= 0.0065, Training Accuracy= 1.000\n",
      "Step 28400, Minibatch Loss= 0.0059, Training Accuracy= 1.000\n",
      "Step 28600, Minibatch Loss= 0.0062, Training Accuracy= 1.000\n",
      "Step 28800, Minibatch Loss= 0.0044, Training Accuracy= 1.000\n",
      "Step 29000, Minibatch Loss= 0.0055, Training Accuracy= 1.000\n",
      "Step 29200, Minibatch Loss= 0.0059, Training Accuracy= 1.000\n",
      "Step 29400, Minibatch Loss= 0.0054, Training Accuracy= 1.000\n",
      "Step 29600, Minibatch Loss= 0.0057, Training Accuracy= 1.000\n",
      "Step 29800, Minibatch Loss= 0.0040, Training Accuracy= 1.000\n",
      "Step 30000, Minibatch Loss= 0.0050, Training Accuracy= 1.000\n",
      "Step 30200, Minibatch Loss= 0.0054, Training Accuracy= 1.000\n",
      "Step 30400, Minibatch Loss= 0.0049, Training Accuracy= 1.000\n",
      "Step 30600, Minibatch Loss= 0.0052, Training Accuracy= 1.000\n",
      "Step 30800, Minibatch Loss= 0.0037, Training Accuracy= 1.000\n",
      "Step 31000, Minibatch Loss= 0.0046, Training Accuracy= 1.000\n",
      "Step 31200, Minibatch Loss= 0.0049, Training Accuracy= 1.000\n",
      "Step 31400, Minibatch Loss= 0.0045, Training Accuracy= 1.000\n",
      "Step 31600, Minibatch Loss= 0.0048, Training Accuracy= 1.000\n",
      "Step 31800, Minibatch Loss= 0.0034, Training Accuracy= 1.000\n",
      "Step 32000, Minibatch Loss= 0.0042, Training Accuracy= 1.000\n",
      "Step 32200, Minibatch Loss= 0.0045, Training Accuracy= 1.000\n",
      "Step 32400, Minibatch Loss= 0.0042, Training Accuracy= 1.000\n",
      "Step 32600, Minibatch Loss= 0.0045, Training Accuracy= 1.000\n",
      "Step 32800, Minibatch Loss= 0.0031, Training Accuracy= 1.000\n",
      "Step 33000, Minibatch Loss= 0.0039, Training Accuracy= 1.000\n",
      "Step 33200, Minibatch Loss= 0.0042, Training Accuracy= 1.000\n",
      "Step 33400, Minibatch Loss= 0.0039, Training Accuracy= 1.000\n",
      "Step 33600, Minibatch Loss= 0.0041, Training Accuracy= 1.000\n",
      "Step 33800, Minibatch Loss= 0.0029, Training Accuracy= 1.000\n",
      "Step 34000, Minibatch Loss= 0.0036, Training Accuracy= 1.000\n",
      "Step 34200, Minibatch Loss= 0.0039, Training Accuracy= 1.000\n",
      "Step 34400, Minibatch Loss= 0.0036, Training Accuracy= 1.000\n",
      "Step 34600, Minibatch Loss= 0.0039, Training Accuracy= 1.000\n",
      "Step 34800, Minibatch Loss= 0.0027, Training Accuracy= 1.000\n",
      "Step 35000, Minibatch Loss= 0.0033, Training Accuracy= 1.000\n",
      "Step 35200, Minibatch Loss= 0.0036, Training Accuracy= 1.000\n",
      "Step 35400, Minibatch Loss= 0.0034, Training Accuracy= 1.000\n",
      "Step 35600, Minibatch Loss= 0.0036, Training Accuracy= 1.000\n",
      "Step 35800, Minibatch Loss= 0.0025, Training Accuracy= 1.000\n",
      "Step 36000, Minibatch Loss= 0.0031, Training Accuracy= 1.000\n",
      "Step 36200, Minibatch Loss= 0.0034, Training Accuracy= 1.000\n",
      "Step 36400, Minibatch Loss= 0.0032, Training Accuracy= 1.000\n",
      "Step 36600, Minibatch Loss= 0.0034, Training Accuracy= 1.000\n",
      "Step 36800, Minibatch Loss= 0.0023, Training Accuracy= 1.000\n",
      "Step 37000, Minibatch Loss= 0.0029, Training Accuracy= 1.000\n",
      "Step 37200, Minibatch Loss= 0.0032, Training Accuracy= 1.000\n",
      "Step 37400, Minibatch Loss= 0.0030, Training Accuracy= 1.000\n",
      "Step 37600, Minibatch Loss= 0.0032, Training Accuracy= 1.000\n",
      "Step 37800, Minibatch Loss= 0.0022, Training Accuracy= 1.000\n",
      "Step 38000, Minibatch Loss= 0.0027, Training Accuracy= 1.000\n",
      "Step 38200, Minibatch Loss= 0.0030, Training Accuracy= 1.000\n",
      "Step 38400, Minibatch Loss= 0.0028, Training Accuracy= 1.000\n",
      "Step 38600, Minibatch Loss= 0.0030, Training Accuracy= 1.000\n",
      "Step 38800, Minibatch Loss= 0.0021, Training Accuracy= 1.000\n",
      "Step 39000, Minibatch Loss= 0.0026, Training Accuracy= 1.000\n",
      "Step 39200, Minibatch Loss= 0.0028, Training Accuracy= 1.000\n",
      "Step 39400, Minibatch Loss= 0.0027, Training Accuracy= 1.000\n",
      "Step 39600, Minibatch Loss= 0.0029, Training Accuracy= 1.000\n",
      "Step 39800, Minibatch Loss= 0.0020, Training Accuracy= 1.000\n",
      "Step 40000, Minibatch Loss= 0.0024, Training Accuracy= 1.000\n",
      "Step 40200, Minibatch Loss= 0.0027, Training Accuracy= 1.000\n",
      "Step 40400, Minibatch Loss= 0.0025, Training Accuracy= 1.000\n",
      "Step 40600, Minibatch Loss= 0.0027, Training Accuracy= 1.000\n",
      "Step 40800, Minibatch Loss= 0.0019, Training Accuracy= 1.000\n",
      "Step 41000, Minibatch Loss= 0.0023, Training Accuracy= 1.000\n",
      "Step 41200, Minibatch Loss= 0.0026, Training Accuracy= 1.000\n",
      "Step 41400, Minibatch Loss= 0.0024, Training Accuracy= 1.000\n",
      "Step 41600, Minibatch Loss= 0.0026, Training Accuracy= 1.000\n",
      "Step 41800, Minibatch Loss= 0.0018, Training Accuracy= 1.000\n",
      "Step 42000, Minibatch Loss= 0.0022, Training Accuracy= 1.000\n",
      "Step 42200, Minibatch Loss= 0.0024, Training Accuracy= 1.000\n",
      "Step 42400, Minibatch Loss= 0.0023, Training Accuracy= 1.000\n",
      "Step 42600, Minibatch Loss= 0.0025, Training Accuracy= 1.000\n",
      "Step 42800, Minibatch Loss= 0.0017, Training Accuracy= 1.000\n",
      "Step 43000, Minibatch Loss= 0.0021, Training Accuracy= 1.000\n",
      "Step 43200, Minibatch Loss= 0.0023, Training Accuracy= 1.000\n",
      "Step 43400, Minibatch Loss= 0.0022, Training Accuracy= 1.000\n",
      "Step 43600, Minibatch Loss= 0.0024, Training Accuracy= 1.000\n",
      "Step 43800, Minibatch Loss= 0.0016, Training Accuracy= 1.000\n",
      "Step 44000, Minibatch Loss= 0.0020, Training Accuracy= 1.000\n",
      "Step 44200, Minibatch Loss= 0.0022, Training Accuracy= 1.000\n",
      "Step 44400, Minibatch Loss= 0.0021, Training Accuracy= 1.000\n",
      "Step 44600, Minibatch Loss= 0.0023, Training Accuracy= 1.000\n",
      "Step 44800, Minibatch Loss= 0.0015, Training Accuracy= 1.000\n",
      "Step 45000, Minibatch Loss= 0.0019, Training Accuracy= 1.000\n",
      "Step 45200, Minibatch Loss= 0.0021, Training Accuracy= 1.000\n",
      "Step 45400, Minibatch Loss= 0.0020, Training Accuracy= 1.000\n",
      "Step 45600, Minibatch Loss= 0.0022, Training Accuracy= 1.000\n",
      "Step 45800, Minibatch Loss= 0.0015, Training Accuracy= 1.000\n",
      "Step 46000, Minibatch Loss= 0.0018, Training Accuracy= 1.000\n",
      "Step 46200, Minibatch Loss= 0.0020, Training Accuracy= 1.000\n",
      "Step 46400, Minibatch Loss= 0.0019, Training Accuracy= 1.000\n",
      "Step 46600, Minibatch Loss= 0.0021, Training Accuracy= 1.000\n",
      "Step 46800, Minibatch Loss= 0.0014, Training Accuracy= 1.000\n",
      "Step 47000, Minibatch Loss= 0.0017, Training Accuracy= 1.000\n",
      "Step 47200, Minibatch Loss= 0.0019, Training Accuracy= 1.000\n",
      "Step 47400, Minibatch Loss= 0.0018, Training Accuracy= 1.000\n",
      "Step 47600, Minibatch Loss= 0.0020, Training Accuracy= 1.000\n",
      "Step 47800, Minibatch Loss= 0.0013, Training Accuracy= 1.000\n",
      "Step 48000, Minibatch Loss= 0.0017, Training Accuracy= 1.000\n",
      "Step 48200, Minibatch Loss= 0.0019, Training Accuracy= 1.000\n",
      "Step 48400, Minibatch Loss= 0.0018, Training Accuracy= 1.000\n",
      "Step 48600, Minibatch Loss= 0.0019, Training Accuracy= 1.000\n",
      "Step 48800, Minibatch Loss= 0.0013, Training Accuracy= 1.000\n",
      "Step 49000, Minibatch Loss= 0.0016, Training Accuracy= 1.000\n",
      "Step 49200, Minibatch Loss= 0.0018, Training Accuracy= 1.000\n",
      "Step 49400, Minibatch Loss= 0.0017, Training Accuracy= 1.000\n",
      "Step 49600, Minibatch Loss= 0.0019, Training Accuracy= 1.000\n",
      "Step 49800, Minibatch Loss= 0.0012, Training Accuracy= 1.000\n",
      "Step 50000, Minibatch Loss= 0.0015, Training Accuracy= 1.000\n",
      "Step 50200, Minibatch Loss= 0.0017, Training Accuracy= 1.000\n",
      "Step 50400, Minibatch Loss= 0.0016, Training Accuracy= 1.000\n",
      "Step 50600, Minibatch Loss= 0.0018, Training Accuracy= 1.000\n",
      "Step 50800, Minibatch Loss= 0.0012, Training Accuracy= 1.000\n",
      "Step 51000, Minibatch Loss= 0.0015, Training Accuracy= 1.000\n",
      "Step 51200, Minibatch Loss= 0.0017, Training Accuracy= 1.000\n",
      "Step 51400, Minibatch Loss= 0.0016, Training Accuracy= 1.000\n",
      "Step 51600, Minibatch Loss= 0.0017, Training Accuracy= 1.000\n",
      "Step 51800, Minibatch Loss= 0.0011, Training Accuracy= 1.000\n",
      "Step 52000, Minibatch Loss= 0.0014, Training Accuracy= 1.000\n",
      "Step 52200, Minibatch Loss= 0.0016, Training Accuracy= 1.000\n",
      "Step 52400, Minibatch Loss= 0.0015, Training Accuracy= 1.000\n",
      "Step 52600, Minibatch Loss= 0.0017, Training Accuracy= 1.000\n",
      "Step 52800, Minibatch Loss= 0.0011, Training Accuracy= 1.000\n",
      "Step 53000, Minibatch Loss= 0.0014, Training Accuracy= 1.000\n",
      "Step 53200, Minibatch Loss= 0.0015, Training Accuracy= 1.000\n",
      "Step 53400, Minibatch Loss= 0.0015, Training Accuracy= 1.000\n",
      "Step 53600, Minibatch Loss= 0.0016, Training Accuracy= 1.000\n",
      "Step 53800, Minibatch Loss= 0.0011, Training Accuracy= 1.000\n",
      "Step 54000, Minibatch Loss= 0.0013, Training Accuracy= 1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 54200, Minibatch Loss= 0.0015, Training Accuracy= 1.000\n",
      "Step 54400, Minibatch Loss= 0.0014, Training Accuracy= 1.000\n",
      "Step 54600, Minibatch Loss= 0.0016, Training Accuracy= 1.000\n",
      "Step 54800, Minibatch Loss= 0.0010, Training Accuracy= 1.000\n",
      "Step 55000, Minibatch Loss= 0.0013, Training Accuracy= 1.000\n",
      "Step 55200, Minibatch Loss= 0.0014, Training Accuracy= 1.000\n",
      "Step 55400, Minibatch Loss= 0.0014, Training Accuracy= 1.000\n",
      "Step 55600, Minibatch Loss= 0.0015, Training Accuracy= 1.000\n",
      "Step 55800, Minibatch Loss= 0.0010, Training Accuracy= 1.000\n",
      "Step 56000, Minibatch Loss= 0.0012, Training Accuracy= 1.000\n",
      "Step 56200, Minibatch Loss= 0.0014, Training Accuracy= 1.000\n",
      "Step 56400, Minibatch Loss= 0.0013, Training Accuracy= 1.000\n",
      "Step 56600, Minibatch Loss= 0.0015, Training Accuracy= 1.000\n",
      "Step 56800, Minibatch Loss= 0.0010, Training Accuracy= 1.000\n",
      "Step 57000, Minibatch Loss= 0.0012, Training Accuracy= 1.000\n",
      "Step 57200, Minibatch Loss= 0.0013, Training Accuracy= 1.000\n",
      "Step 57400, Minibatch Loss= 0.0013, Training Accuracy= 1.000\n",
      "Step 57600, Minibatch Loss= 0.0014, Training Accuracy= 1.000\n",
      "Step 57800, Minibatch Loss= 0.0009, Training Accuracy= 1.000\n",
      "Step 58000, Minibatch Loss= 0.0012, Training Accuracy= 1.000\n",
      "Step 58200, Minibatch Loss= 0.0013, Training Accuracy= 1.000\n",
      "Step 58400, Minibatch Loss= 0.0013, Training Accuracy= 1.000\n",
      "Step 58600, Minibatch Loss= 0.0014, Training Accuracy= 1.000\n",
      "Step 58800, Minibatch Loss= 0.0009, Training Accuracy= 1.000\n",
      "Step 59000, Minibatch Loss= 0.0011, Training Accuracy= 1.000\n",
      "Step 59200, Minibatch Loss= 0.0013, Training Accuracy= 1.000\n",
      "Step 59400, Minibatch Loss= 0.0012, Training Accuracy= 1.000\n",
      "Step 59600, Minibatch Loss= 0.0013, Training Accuracy= 1.000\n",
      "Step 59800, Minibatch Loss= 0.0009, Training Accuracy= 1.000\n",
      "Step 60000, Minibatch Loss= 0.0011, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "('Testing Accuracy:', 1.0)\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    # tf Graph input\n",
    "    input_x = tf.placeholder(\"float\", [None, timesteps, num_input])\n",
    "    input_y = tf.placeholder(\"float\", [None, num_classes])\n",
    "    # Define weights\n",
    "    weights = {\n",
    "        'out': tf.Variable(tf.random_normal([num_hidden, num_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "    }\n",
    "    \n",
    "    logits = RNN(input_x, weights, biases)\n",
    "    prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits=logits, labels=input_y))\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "    train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "    # Evaluate model (with test logits, for dropout to be disabled)\n",
    "    correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(input_y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "    # Initialize the variables (i.e. assign their default value)\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        # Run the initializer\n",
    "        sess.run(init)\n",
    "        index_gen = batch_gen(batch_size= batch_size)\n",
    "        for step in range(1, training_steps+1):\n",
    "            \n",
    "#             batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            index = next(index_gen)\n",
    "            batch_x = X[index]\n",
    "            batch_y = mnist_encoder(y[index])\n",
    "            # Reshape data to get 28 seq of 28 elements\n",
    "            batch_x = batch_x.reshape((batch_size, timesteps, num_input))\n",
    "            # Run optimization op (backprop)\n",
    "            sess.run(train_op, feed_dict={input_x: batch_x, input_y: batch_y})\n",
    "            if step % display_step == 0 or step == 1:\n",
    "                # Calculate batch loss and accuracy\n",
    "                loss, acc = sess.run([loss_op, accuracy], feed_dict={input_x: batch_x,\n",
    "                                                                     input_y: batch_y})\n",
    "                print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "                      \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                      \"{:.3f}\".format(acc))\n",
    "\n",
    "        print(\"Optimization Finished!\")\n",
    "\n",
    "        # Calculate accuracy for 128 mnist test images\n",
    "        test_len = 128\n",
    "        test_data = X[:test_len].reshape((-1, timesteps, num_input))\n",
    "        test_label = mnist_encoder(y[:test_len])\n",
    "        print(\"Testing Accuracy:\", \\\n",
    "            sess.run(accuracy, feed_dict={input_x: test_data, input_y: test_label}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 1)\n",
      "step: 1 ,batch_cost= 1.3125 ,accuray= 0.520\n",
      "step: 2 ,batch_cost= 1.7398 ,accuray= 0.340\n",
      "step: 3 ,batch_cost= 1.8699 ,accuray= 0.240\n",
      "step: 4 ,batch_cost= 1.4470 ,accuray= 0.440\n",
      "step: 5 ,batch_cost= 1.2081 ,accuray= 0.520\n",
      "step: 6 ,batch_cost= 1.4125 ,accuray= 0.400\n",
      "step: 7 ,batch_cost= 1.6736 ,accuray= 0.320\n",
      "step: 8 ,batch_cost= 1.5873 ,accuray= 0.320\n",
      "step: 9 ,batch_cost= 1.3896 ,accuray= 0.380\n",
      "step: 10 ,batch_cost= 1.0856 ,accuray= 0.520\n",
      "step: 11 ,batch_cost= 1.1102 ,accuray= 0.520\n",
      "step: 12 ,batch_cost= 1.4436 ,accuray= 0.340\n",
      "step: 13 ,batch_cost= 1.5374 ,accuray= 0.240\n",
      "step: 14 ,batch_cost= 1.2168 ,accuray= 0.440\n",
      "step: 15 ,batch_cost= 1.0220 ,accuray= 0.520\n",
      "step: 16 ,batch_cost= 1.1795 ,accuray= 0.400\n",
      "step: 17 ,batch_cost= 1.3978 ,accuray= 0.320\n",
      "step: 18 ,batch_cost= 1.3183 ,accuray= 0.320\n",
      "step: 19 ,batch_cost= 1.1624 ,accuray= 0.380\n",
      "step: 20 ,batch_cost= 0.9294 ,accuray= 0.520\n",
      "step: 21 ,batch_cost= 0.9586 ,accuray= 0.520\n",
      "step: 22 ,batch_cost= 1.2112 ,accuray= 0.340\n",
      "step: 23 ,batch_cost= 1.2763 ,accuray= 0.240\n",
      "step: 24 ,batch_cost= 1.0436 ,accuray= 0.440\n",
      "step: 25 ,batch_cost= 0.8869 ,accuray= 0.520\n",
      "step: 26 ,batch_cost= 1.0066 ,accuray= 0.400\n",
      "step: 27 ,batch_cost= 1.1853 ,accuray= 0.320\n",
      "step: 28 ,batch_cost= 1.1151 ,accuray= 0.320\n",
      "step: 29 ,batch_cost= 0.9963 ,accuray= 0.380\n",
      "step: 30 ,batch_cost= 0.8220 ,accuray= 0.520\n",
      "step: 31 ,batch_cost= 0.8545 ,accuray= 0.520\n",
      "step: 32 ,batch_cost= 1.0398 ,accuray= 0.340\n",
      "step: 33 ,batch_cost= 1.0821 ,accuray= 0.240\n",
      "step: 34 ,batch_cost= 0.9223 ,accuray= 0.440\n",
      "step: 35 ,batch_cost= 0.7970 ,accuray= 0.520\n",
      "step: 36 ,batch_cost= 0.8862 ,accuray= 0.400\n",
      "step: 37 ,batch_cost= 1.0299 ,accuray= 0.320\n",
      "step: 38 ,batch_cost= 0.9694 ,accuray= 0.320\n",
      "step: 39 ,batch_cost= 0.8816 ,accuray= 0.380\n",
      "step: 40 ,batch_cost= 0.7545 ,accuray= 0.520\n",
      "step: 41 ,batch_cost= 0.7888 ,accuray= 0.520\n",
      "step: 42 ,batch_cost= 0.9190 ,accuray= 0.340\n",
      "step: 43 ,batch_cost= 0.9430 ,accuray= 0.240\n",
      "step: 44 ,batch_cost= 0.8420 ,accuray= 0.440\n",
      "step: 45 ,batch_cost= 0.7417 ,accuray= 0.520\n",
      "step: 46 ,batch_cost= 0.8064 ,accuray= 0.400\n",
      "step: 47 ,batch_cost= 0.9202 ,accuray= 0.320\n",
      "step: 48 ,batch_cost= 0.8684 ,accuray= 0.320\n",
      "step: 49 ,batch_cost= 0.8054 ,accuray= 0.400\n",
      "step: 50 ,batch_cost= 0.7155 ,accuray= 0.500\n",
      "step: 51 ,batch_cost= 0.7504 ,accuray= 0.520\n",
      "step: 52 ,batch_cost= 0.8362 ,accuray= 0.340\n",
      "step: 53 ,batch_cost= 0.8453 ,accuray= 0.280\n",
      "step: 54 ,batch_cost= 0.7910 ,accuray= 0.440\n",
      "step: 55 ,batch_cost= 0.7101 ,accuray= 0.520\n",
      "step: 56 ,batch_cost= 0.7550 ,accuray= 0.420\n",
      "step: 57 ,batch_cost= 0.8439 ,accuray= 0.320\n",
      "step: 58 ,batch_cost= 0.7994 ,accuray= 0.320\n",
      "step: 59 ,batch_cost= 0.7559 ,accuray= 0.420\n",
      "step: 60 ,batch_cost= 0.6950 ,accuray= 0.540\n",
      "step: 61 ,batch_cost= 0.7298 ,accuray= 0.540\n",
      "step: 62 ,batch_cost= 0.7800 ,accuray= 0.360\n",
      "step: 63 ,batch_cost= 0.7768 ,accuray= 0.360\n",
      "step: 64 ,batch_cost= 0.7595 ,accuray= 0.420\n",
      "step: 65 ,batch_cost= 0.6937 ,accuray= 0.540\n",
      "step: 66 ,batch_cost= 0.7226 ,accuray= 0.500\n",
      "step: 67 ,batch_cost= 0.7911 ,accuray= 0.320\n",
      "step: 68 ,batch_cost= 0.7523 ,accuray= 0.340\n",
      "step: 69 ,batch_cost= 0.7241 ,accuray= 0.480\n",
      "step: 70 ,batch_cost= 0.6860 ,accuray= 0.520\n",
      "step: 71 ,batch_cost= 0.7200 ,accuray= 0.520\n",
      "step: 72 ,batch_cost= 0.7418 ,accuray= 0.400\n",
      "step: 73 ,batch_cost= 0.7286 ,accuray= 0.420\n",
      "step: 74 ,batch_cost= 0.7404 ,accuray= 0.460\n",
      "step: 75 ,batch_cost= 0.6863 ,accuray= 0.540\n",
      "step: 76 ,batch_cost= 0.7022 ,accuray= 0.620\n",
      "step: 77 ,batch_cost= 0.7543 ,accuray= 0.340\n",
      "step: 78 ,batch_cost= 0.7200 ,accuray= 0.460\n",
      "step: 79 ,batch_cost= 0.7035 ,accuray= 0.520\n",
      "step: 80 ,batch_cost= 0.6834 ,accuray= 0.520\n",
      "step: 81 ,batch_cost= 0.7166 ,accuray= 0.540\n",
      "step: 82 ,batch_cost= 0.7156 ,accuray= 0.440\n",
      "step: 83 ,batch_cost= 0.6942 ,accuray= 0.540\n",
      "step: 84 ,batch_cost= 0.7290 ,accuray= 0.400\n",
      "step: 85 ,batch_cost= 0.6841 ,accuray= 0.540\n",
      "step: 86 ,batch_cost= 0.6895 ,accuray= 0.560\n",
      "step: 87 ,batch_cost= 0.7283 ,accuray= 0.400\n",
      "step: 88 ,batch_cost= 0.6975 ,accuray= 0.500\n",
      "step: 89 ,batch_cost= 0.6903 ,accuray= 0.520\n",
      "step: 90 ,batch_cost= 0.6843 ,accuray= 0.560\n",
      "step: 91 ,batch_cost= 0.7166 ,accuray= 0.480\n",
      "step: 92 ,batch_cost= 0.6975 ,accuray= 0.480\n",
      "step: 93 ,batch_cost= 0.6693 ,accuray= 0.540\n",
      "step: 94 ,batch_cost= 0.7224 ,accuray= 0.480\n",
      "step: 95 ,batch_cost= 0.6848 ,accuray= 0.540\n",
      "step: 96 ,batch_cost= 0.6816 ,accuray= 0.580\n",
      "step: 97 ,batch_cost= 0.7097 ,accuray= 0.500\n",
      "step: 98 ,batch_cost= 0.6816 ,accuray= 0.600\n",
      "step: 99 ,batch_cost= 0.6816 ,accuray= 0.480\n",
      "step: 100 ,batch_cost= 0.6869 ,accuray= 0.540\n",
      "step: 101 ,batch_cost= 0.7184 ,accuray= 0.420\n",
      "step: 102 ,batch_cost= 0.6847 ,accuray= 0.500\n",
      "step: 103 ,batch_cost= 0.6510 ,accuray= 0.680\n",
      "step: 104 ,batch_cost= 0.7187 ,accuray= 0.480\n",
      "step: 105 ,batch_cost= 0.6868 ,accuray= 0.520\n",
      "step: 106 ,batch_cost= 0.6767 ,accuray= 0.520\n",
      "step: 107 ,batch_cost= 0.6961 ,accuray= 0.480\n",
      "step: 108 ,batch_cost= 0.6702 ,accuray= 0.660\n",
      "step: 109 ,batch_cost= 0.6759 ,accuray= 0.540\n",
      "step: 110 ,batch_cost= 0.6902 ,accuray= 0.600\n",
      "step: 111 ,batch_cost= 0.7210 ,accuray= 0.360\n",
      "step: 112 ,batch_cost= 0.6755 ,accuray= 0.560\n",
      "step: 113 ,batch_cost= 0.6373 ,accuray= 0.700\n",
      "step: 114 ,batch_cost= 0.7167 ,accuray= 0.460\n",
      "step: 115 ,batch_cost= 0.6894 ,accuray= 0.580\n",
      "step: 116 ,batch_cost= 0.6737 ,accuray= 0.540\n",
      "step: 117 ,batch_cost= 0.6862 ,accuray= 0.560\n",
      "step: 118 ,batch_cost= 0.6618 ,accuray= 0.660\n",
      "step: 119 ,batch_cost= 0.6722 ,accuray= 0.580\n",
      "step: 120 ,batch_cost= 0.6937 ,accuray= 0.580\n",
      "step: 121 ,batch_cost= 0.7238 ,accuray= 0.400\n",
      "step: 122 ,batch_cost= 0.6688 ,accuray= 0.600\n",
      "step: 123 ,batch_cost= 0.6270 ,accuray= 0.720\n",
      "step: 124 ,batch_cost= 0.7158 ,accuray= 0.440\n",
      "step: 125 ,batch_cost= 0.6922 ,accuray= 0.540\n",
      "step: 126 ,batch_cost= 0.6719 ,accuray= 0.540\n",
      "step: 127 ,batch_cost= 0.6787 ,accuray= 0.520\n",
      "step: 128 ,batch_cost= 0.6557 ,accuray= 0.680\n",
      "step: 129 ,batch_cost= 0.6697 ,accuray= 0.580\n",
      "step: 130 ,batch_cost= 0.6970 ,accuray= 0.620\n",
      "step: 131 ,batch_cost= 0.7266 ,accuray= 0.420\n",
      "step: 132 ,batch_cost= 0.6639 ,accuray= 0.620\n",
      "step: 133 ,batch_cost= 0.6191 ,accuray= 0.720\n",
      "step: 134 ,batch_cost= 0.7154 ,accuray= 0.480\n",
      "step: 135 ,batch_cost= 0.6948 ,accuray= 0.540\n",
      "step: 136 ,batch_cost= 0.6708 ,accuray= 0.520\n",
      "step: 137 ,batch_cost= 0.6731 ,accuray= 0.580\n",
      "step: 138 ,batch_cost= 0.6510 ,accuray= 0.620\n",
      "step: 139 ,batch_cost= 0.6680 ,accuray= 0.600\n",
      "step: 140 ,batch_cost= 0.7000 ,accuray= 0.560\n",
      "step: 141 ,batch_cost= 0.7291 ,accuray= 0.460\n",
      "step: 142 ,batch_cost= 0.6602 ,accuray= 0.640\n",
      "step: 143 ,batch_cost= 0.6129 ,accuray= 0.740\n",
      "step: 144 ,batch_cost= 0.7154 ,accuray= 0.500\n",
      "step: 145 ,batch_cost= 0.6972 ,accuray= 0.520\n",
      "step: 146 ,batch_cost= 0.6701 ,accuray= 0.500\n",
      "step: 147 ,batch_cost= 0.6687 ,accuray= 0.600\n",
      "step: 148 ,batch_cost= 0.6475 ,accuray= 0.640\n",
      "step: 149 ,batch_cost= 0.6668 ,accuray= 0.640\n",
      "step: 150 ,batch_cost= 0.7027 ,accuray= 0.540\n",
      "step: 151 ,batch_cost= 0.7314 ,accuray= 0.440\n",
      "step: 152 ,batch_cost= 0.6574 ,accuray= 0.680\n",
      "step: 153 ,batch_cost= 0.6082 ,accuray= 0.740\n",
      "step: 154 ,batch_cost= 0.7155 ,accuray= 0.520\n",
      "step: 155 ,batch_cost= 0.6994 ,accuray= 0.520\n",
      "step: 156 ,batch_cost= 0.6698 ,accuray= 0.520\n",
      "step: 157 ,batch_cost= 0.6654 ,accuray= 0.640\n",
      "step: 158 ,batch_cost= 0.6448 ,accuray= 0.620\n",
      "step: 159 ,batch_cost= 0.6660 ,accuray= 0.640\n",
      "step: 160 ,batch_cost= 0.7050 ,accuray= 0.500\n",
      "step: 161 ,batch_cost= 0.7333 ,accuray= 0.440\n",
      "step: 162 ,batch_cost= 0.6553 ,accuray= 0.700\n",
      "step: 163 ,batch_cost= 0.6044 ,accuray= 0.740\n",
      "step: 164 ,batch_cost= 0.7158 ,accuray= 0.520\n",
      "step: 165 ,batch_cost= 0.7012 ,accuray= 0.540\n",
      "step: 166 ,batch_cost= 0.6696 ,accuray= 0.520\n",
      "step: 167 ,batch_cost= 0.6627 ,accuray= 0.640\n",
      "step: 168 ,batch_cost= 0.6427 ,accuray= 0.620\n",
      "step: 169 ,batch_cost= 0.6655 ,accuray= 0.640\n",
      "step: 170 ,batch_cost= 0.7069 ,accuray= 0.480\n",
      "step: 171 ,batch_cost= 0.7350 ,accuray= 0.460\n",
      "step: 172 ,batch_cost= 0.6536 ,accuray= 0.700\n",
      "step: 173 ,batch_cost= 0.6014 ,accuray= 0.740\n",
      "step: 174 ,batch_cost= 0.7160 ,accuray= 0.520\n",
      "step: 175 ,batch_cost= 0.7028 ,accuray= 0.540\n",
      "step: 176 ,batch_cost= 0.6695 ,accuray= 0.540\n",
      "step: 177 ,batch_cost= 0.6607 ,accuray= 0.680\n",
      "step: 178 ,batch_cost= 0.6411 ,accuray= 0.620\n",
      "step: 179 ,batch_cost= 0.6651 ,accuray= 0.640\n",
      "step: 180 ,batch_cost= 0.7086 ,accuray= 0.480\n",
      "step: 181 ,batch_cost= 0.7364 ,accuray= 0.420\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 182 ,batch_cost= 0.6523 ,accuray= 0.700\n",
      "step: 183 ,batch_cost= 0.5991 ,accuray= 0.740\n",
      "step: 184 ,batch_cost= 0.7163 ,accuray= 0.520\n",
      "step: 185 ,batch_cost= 0.7042 ,accuray= 0.540\n",
      "step: 186 ,batch_cost= 0.6695 ,accuray= 0.560\n",
      "step: 187 ,batch_cost= 0.6590 ,accuray= 0.680\n",
      "step: 188 ,batch_cost= 0.6398 ,accuray= 0.620\n",
      "step: 189 ,batch_cost= 0.6648 ,accuray= 0.640\n",
      "step: 190 ,batch_cost= 0.7100 ,accuray= 0.480\n",
      "step: 191 ,batch_cost= 0.7376 ,accuray= 0.420\n",
      "step: 192 ,batch_cost= 0.6513 ,accuray= 0.680\n",
      "step: 193 ,batch_cost= 0.5972 ,accuray= 0.740\n",
      "step: 194 ,batch_cost= 0.7165 ,accuray= 0.520\n",
      "step: 195 ,batch_cost= 0.7053 ,accuray= 0.540\n",
      "step: 196 ,batch_cost= 0.6696 ,accuray= 0.560\n",
      "step: 197 ,batch_cost= 0.6577 ,accuray= 0.700\n",
      "step: 198 ,batch_cost= 0.6387 ,accuray= 0.620\n",
      "step: 199 ,batch_cost= 0.6646 ,accuray= 0.640\n",
      "step: 201 ,batch_cost= 0.7385 ,accuray= 0.420\n",
      "step: 202 ,batch_cost= 0.6504 ,accuray= 0.680\n",
      "step: 203 ,batch_cost= 0.5956 ,accuray= 0.740\n",
      "step: 204 ,batch_cost= 0.7167 ,accuray= 0.520\n",
      "step: 205 ,batch_cost= 0.7062 ,accuray= 0.520\n",
      "step: 206 ,batch_cost= 0.6696 ,accuray= 0.560\n",
      "step: 207 ,batch_cost= 0.6567 ,accuray= 0.700\n",
      "step: 208 ,batch_cost= 0.6379 ,accuray= 0.620\n",
      "step: 209 ,batch_cost= 0.6645 ,accuray= 0.640\n",
      "step: 210 ,batch_cost= 0.7121 ,accuray= 0.480\n",
      "step: 211 ,batch_cost= 0.7393 ,accuray= 0.420\n",
      "step: 212 ,batch_cost= 0.6498 ,accuray= 0.680\n",
      "step: 213 ,batch_cost= 0.5944 ,accuray= 0.760\n",
      "step: 214 ,batch_cost= 0.7168 ,accuray= 0.520\n",
      "step: 215 ,batch_cost= 0.7070 ,accuray= 0.520\n",
      "step: 216 ,batch_cost= 0.6696 ,accuray= 0.560\n",
      "step: 217 ,batch_cost= 0.6558 ,accuray= 0.700\n",
      "step: 218 ,batch_cost= 0.6372 ,accuray= 0.620\n",
      "step: 219 ,batch_cost= 0.6644 ,accuray= 0.640\n",
      "step: 220 ,batch_cost= 0.7129 ,accuray= 0.480\n",
      "step: 221 ,batch_cost= 0.7400 ,accuray= 0.420\n",
      "step: 222 ,batch_cost= 0.6492 ,accuray= 0.680\n",
      "step: 223 ,batch_cost= 0.5934 ,accuray= 0.760\n",
      "step: 224 ,batch_cost= 0.7169 ,accuray= 0.520\n",
      "step: 225 ,batch_cost= 0.7077 ,accuray= 0.500\n",
      "step: 226 ,batch_cost= 0.6697 ,accuray= 0.560\n",
      "step: 227 ,batch_cost= 0.6551 ,accuray= 0.700\n",
      "step: 228 ,batch_cost= 0.6366 ,accuray= 0.620\n",
      "step: 229 ,batch_cost= 0.6643 ,accuray= 0.620\n",
      "step: 230 ,batch_cost= 0.7136 ,accuray= 0.480\n",
      "step: 231 ,batch_cost= 0.7405 ,accuray= 0.420\n",
      "step: 232 ,batch_cost= 0.6487 ,accuray= 0.680\n",
      "step: 233 ,batch_cost= 0.5926 ,accuray= 0.760\n",
      "step: 234 ,batch_cost= 0.7170 ,accuray= 0.500\n",
      "step: 235 ,batch_cost= 0.7082 ,accuray= 0.500\n",
      "step: 236 ,batch_cost= 0.6697 ,accuray= 0.560\n",
      "step: 237 ,batch_cost= 0.6546 ,accuray= 0.680\n",
      "step: 238 ,batch_cost= 0.6362 ,accuray= 0.620\n",
      "step: 239 ,batch_cost= 0.6642 ,accuray= 0.620\n",
      "step: 240 ,batch_cost= 0.7141 ,accuray= 0.480\n",
      "step: 241 ,batch_cost= 0.7410 ,accuray= 0.440\n",
      "step: 242 ,batch_cost= 0.6484 ,accuray= 0.680\n",
      "step: 243 ,batch_cost= 0.5919 ,accuray= 0.760\n",
      "step: 244 ,batch_cost= 0.7171 ,accuray= 0.500\n",
      "step: 245 ,batch_cost= 0.7087 ,accuray= 0.480\n",
      "step: 246 ,batch_cost= 0.6698 ,accuray= 0.560\n",
      "step: 247 ,batch_cost= 0.6541 ,accuray= 0.680\n",
      "step: 248 ,batch_cost= 0.6358 ,accuray= 0.620\n",
      "step: 249 ,batch_cost= 0.6642 ,accuray= 0.620\n",
      "step: 250 ,batch_cost= 0.7145 ,accuray= 0.480\n",
      "step: 251 ,batch_cost= 0.7413 ,accuray= 0.440\n",
      "step: 252 ,batch_cost= 0.6481 ,accuray= 0.680\n",
      "step: 253 ,batch_cost= 0.5914 ,accuray= 0.760\n",
      "step: 254 ,batch_cost= 0.7171 ,accuray= 0.500\n",
      "step: 255 ,batch_cost= 0.7091 ,accuray= 0.480\n",
      "step: 256 ,batch_cost= 0.6698 ,accuray= 0.560\n",
      "step: 257 ,batch_cost= 0.6537 ,accuray= 0.660\n",
      "step: 258 ,batch_cost= 0.6355 ,accuray= 0.640\n",
      "step: 259 ,batch_cost= 0.6641 ,accuray= 0.620\n",
      "step: 260 ,batch_cost= 0.7149 ,accuray= 0.480\n",
      "step: 261 ,batch_cost= 0.7416 ,accuray= 0.440\n",
      "step: 262 ,batch_cost= 0.6478 ,accuray= 0.680\n",
      "step: 263 ,batch_cost= 0.5910 ,accuray= 0.760\n",
      "step: 264 ,batch_cost= 0.7171 ,accuray= 0.520\n",
      "step: 265 ,batch_cost= 0.7094 ,accuray= 0.480\n",
      "step: 266 ,batch_cost= 0.6698 ,accuray= 0.560\n",
      "step: 267 ,batch_cost= 0.6534 ,accuray= 0.660\n",
      "step: 268 ,batch_cost= 0.6352 ,accuray= 0.640\n",
      "step: 269 ,batch_cost= 0.6641 ,accuray= 0.620\n",
      "step: 270 ,batch_cost= 0.7152 ,accuray= 0.480\n",
      "step: 271 ,batch_cost= 0.7418 ,accuray= 0.440\n",
      "step: 272 ,batch_cost= 0.6476 ,accuray= 0.680\n",
      "step: 273 ,batch_cost= 0.5906 ,accuray= 0.760\n",
      "step: 274 ,batch_cost= 0.7171 ,accuray= 0.520\n",
      "step: 275 ,batch_cost= 0.7096 ,accuray= 0.480\n",
      "step: 276 ,batch_cost= 0.6699 ,accuray= 0.560\n",
      "step: 277 ,batch_cost= 0.6532 ,accuray= 0.660\n",
      "step: 278 ,batch_cost= 0.6350 ,accuray= 0.660\n",
      "step: 279 ,batch_cost= 0.6641 ,accuray= 0.620\n",
      "step: 280 ,batch_cost= 0.7154 ,accuray= 0.480\n",
      "step: 281 ,batch_cost= 0.7420 ,accuray= 0.440\n",
      "step: 282 ,batch_cost= 0.6474 ,accuray= 0.680\n",
      "step: 283 ,batch_cost= 0.5904 ,accuray= 0.760\n",
      "step: 284 ,batch_cost= 0.7171 ,accuray= 0.520\n",
      "step: 285 ,batch_cost= 0.7098 ,accuray= 0.480\n",
      "step: 286 ,batch_cost= 0.6699 ,accuray= 0.560\n",
      "step: 287 ,batch_cost= 0.6530 ,accuray= 0.660\n",
      "step: 288 ,batch_cost= 0.6348 ,accuray= 0.660\n",
      "step: 289 ,batch_cost= 0.6641 ,accuray= 0.620\n",
      "step: 290 ,batch_cost= 0.7156 ,accuray= 0.480\n",
      "step: 291 ,batch_cost= 0.7421 ,accuray= 0.440\n",
      "step: 292 ,batch_cost= 0.6472 ,accuray= 0.680\n",
      "step: 293 ,batch_cost= 0.5901 ,accuray= 0.760\n",
      "step: 294 ,batch_cost= 0.7171 ,accuray= 0.520\n",
      "step: 295 ,batch_cost= 0.7100 ,accuray= 0.480\n",
      "step: 296 ,batch_cost= 0.6699 ,accuray= 0.560\n",
      "step: 297 ,batch_cost= 0.6528 ,accuray= 0.660\n",
      "step: 298 ,batch_cost= 0.6347 ,accuray= 0.660\n",
      "step: 299 ,batch_cost= 0.6640 ,accuray= 0.620\n",
      "step: 300 ,batch_cost= 0.7158 ,accuray= 0.480\n",
      "step: 301 ,batch_cost= 0.7422 ,accuray= 0.440\n",
      "step: 302 ,batch_cost= 0.6471 ,accuray= 0.680\n",
      "step: 303 ,batch_cost= 0.5899 ,accuray= 0.760\n",
      "step: 304 ,batch_cost= 0.7171 ,accuray= 0.520\n",
      "step: 305 ,batch_cost= 0.7101 ,accuray= 0.480\n",
      "step: 306 ,batch_cost= 0.6699 ,accuray= 0.560\n",
      "step: 307 ,batch_cost= 0.6526 ,accuray= 0.660\n",
      "step: 308 ,batch_cost= 0.6345 ,accuray= 0.660\n",
      "step: 309 ,batch_cost= 0.6640 ,accuray= 0.620\n",
      "step: 310 ,batch_cost= 0.7159 ,accuray= 0.480\n",
      "step: 311 ,batch_cost= 0.7423 ,accuray= 0.440\n",
      "step: 312 ,batch_cost= 0.6470 ,accuray= 0.680\n",
      "step: 313 ,batch_cost= 0.5898 ,accuray= 0.760\n",
      "step: 314 ,batch_cost= 0.7170 ,accuray= 0.520\n",
      "step: 315 ,batch_cost= 0.7103 ,accuray= 0.480\n",
      "step: 316 ,batch_cost= 0.6699 ,accuray= 0.560\n",
      "step: 317 ,batch_cost= 0.6525 ,accuray= 0.660\n",
      "step: 318 ,batch_cost= 0.6344 ,accuray= 0.660\n",
      "step: 319 ,batch_cost= 0.6640 ,accuray= 0.620\n",
      "step: 320 ,batch_cost= 0.7160 ,accuray= 0.480\n",
      "step: 321 ,batch_cost= 0.7423 ,accuray= 0.440\n",
      "step: 322 ,batch_cost= 0.6468 ,accuray= 0.680\n",
      "step: 323 ,batch_cost= 0.5897 ,accuray= 0.760\n",
      "step: 324 ,batch_cost= 0.7170 ,accuray= 0.520\n",
      "step: 325 ,batch_cost= 0.7103 ,accuray= 0.480\n",
      "step: 326 ,batch_cost= 0.6699 ,accuray= 0.560\n",
      "step: 327 ,batch_cost= 0.6524 ,accuray= 0.660\n",
      "step: 328 ,batch_cost= 0.6343 ,accuray= 0.660\n",
      "step: 329 ,batch_cost= 0.6640 ,accuray= 0.620\n",
      "step: 330 ,batch_cost= 0.7161 ,accuray= 0.480\n",
      "step: 331 ,batch_cost= 0.7424 ,accuray= 0.440\n",
      "step: 332 ,batch_cost= 0.6467 ,accuray= 0.680\n",
      "step: 333 ,batch_cost= 0.5896 ,accuray= 0.760\n",
      "step: 334 ,batch_cost= 0.7169 ,accuray= 0.520\n",
      "step: 335 ,batch_cost= 0.7104 ,accuray= 0.480\n",
      "step: 336 ,batch_cost= 0.6700 ,accuray= 0.560\n",
      "step: 337 ,batch_cost= 0.6523 ,accuray= 0.680\n",
      "step: 338 ,batch_cost= 0.6342 ,accuray= 0.660\n",
      "step: 339 ,batch_cost= 0.6640 ,accuray= 0.620\n",
      "step: 340 ,batch_cost= 0.7161 ,accuray= 0.480\n",
      "step: 341 ,batch_cost= 0.7424 ,accuray= 0.440\n",
      "step: 342 ,batch_cost= 0.6467 ,accuray= 0.680\n",
      "step: 343 ,batch_cost= 0.5895 ,accuray= 0.760\n",
      "step: 344 ,batch_cost= 0.7169 ,accuray= 0.520\n",
      "step: 345 ,batch_cost= 0.7105 ,accuray= 0.480\n",
      "step: 346 ,batch_cost= 0.6700 ,accuray= 0.560\n",
      "step: 347 ,batch_cost= 0.6522 ,accuray= 0.680\n",
      "step: 348 ,batch_cost= 0.6342 ,accuray= 0.660\n",
      "step: 349 ,batch_cost= 0.6640 ,accuray= 0.620\n",
      "step: 350 ,batch_cost= 0.7162 ,accuray= 0.480\n",
      "step: 351 ,batch_cost= 0.7424 ,accuray= 0.440\n",
      "step: 352 ,batch_cost= 0.6466 ,accuray= 0.680\n",
      "step: 353 ,batch_cost= 0.5894 ,accuray= 0.760\n",
      "step: 354 ,batch_cost= 0.7168 ,accuray= 0.520\n",
      "step: 355 ,batch_cost= 0.7105 ,accuray= 0.480\n",
      "step: 356 ,batch_cost= 0.6700 ,accuray= 0.560\n",
      "step: 357 ,batch_cost= 0.6522 ,accuray= 0.680\n",
      "step: 358 ,batch_cost= 0.6341 ,accuray= 0.660\n",
      "step: 359 ,batch_cost= 0.6640 ,accuray= 0.620\n",
      "step: 360 ,batch_cost= 0.7162 ,accuray= 0.480\n",
      "step: 361 ,batch_cost= 0.7424 ,accuray= 0.440\n",
      "step: 362 ,batch_cost= 0.6465 ,accuray= 0.680\n",
      "step: 363 ,batch_cost= 0.5894 ,accuray= 0.760\n",
      "step: 364 ,batch_cost= 0.7167 ,accuray= 0.520\n",
      "step: 365 ,batch_cost= 0.7106 ,accuray= 0.480\n",
      "step: 366 ,batch_cost= 0.6700 ,accuray= 0.560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 367 ,batch_cost= 0.6521 ,accuray= 0.680\n",
      "step: 368 ,batch_cost= 0.6340 ,accuray= 0.660\n",
      "step: 369 ,batch_cost= 0.6639 ,accuray= 0.620\n",
      "step: 370 ,batch_cost= 0.7163 ,accuray= 0.480\n",
      "step: 371 ,batch_cost= 0.7424 ,accuray= 0.440\n",
      "step: 372 ,batch_cost= 0.6464 ,accuray= 0.680\n",
      "step: 373 ,batch_cost= 0.5893 ,accuray= 0.760\n",
      "step: 374 ,batch_cost= 0.7167 ,accuray= 0.520\n",
      "step: 375 ,batch_cost= 0.7106 ,accuray= 0.480\n",
      "step: 376 ,batch_cost= 0.6700 ,accuray= 0.560\n",
      "step: 377 ,batch_cost= 0.6521 ,accuray= 0.680\n",
      "step: 378 ,batch_cost= 0.6340 ,accuray= 0.660\n",
      "step: 379 ,batch_cost= 0.6639 ,accuray= 0.620\n",
      "step: 380 ,batch_cost= 0.7163 ,accuray= 0.480\n",
      "step: 381 ,batch_cost= 0.7423 ,accuray= 0.440\n",
      "step: 382 ,batch_cost= 0.6464 ,accuray= 0.680\n",
      "step: 383 ,batch_cost= 0.5893 ,accuray= 0.760\n",
      "step: 384 ,batch_cost= 0.7166 ,accuray= 0.520\n",
      "step: 385 ,batch_cost= 0.7106 ,accuray= 0.480\n",
      "step: 386 ,batch_cost= 0.6700 ,accuray= 0.560\n",
      "step: 387 ,batch_cost= 0.6520 ,accuray= 0.680\n",
      "step: 388 ,batch_cost= 0.6339 ,accuray= 0.660\n",
      "step: 389 ,batch_cost= 0.6639 ,accuray= 0.620\n",
      "step: 390 ,batch_cost= 0.7163 ,accuray= 0.480\n",
      "step: 391 ,batch_cost= 0.7423 ,accuray= 0.440\n",
      "step: 392 ,batch_cost= 0.6463 ,accuray= 0.680\n",
      "step: 393 ,batch_cost= 0.5893 ,accuray= 0.760\n",
      "step: 394 ,batch_cost= 0.7165 ,accuray= 0.520\n",
      "step: 395 ,batch_cost= 0.7107 ,accuray= 0.480\n",
      "step: 396 ,batch_cost= 0.6700 ,accuray= 0.560\n",
      "step: 397 ,batch_cost= 0.6520 ,accuray= 0.680\n",
      "step: 398 ,batch_cost= 0.6339 ,accuray= 0.660\n",
      "step: 399 ,batch_cost= 0.6639 ,accuray= 0.620\n",
      "step: 401 ,batch_cost= 0.7423 ,accuray= 0.440\n",
      "step: 402 ,batch_cost= 0.6462 ,accuray= 0.680\n",
      "step: 403 ,batch_cost= 0.5892 ,accuray= 0.760\n",
      "step: 404 ,batch_cost= 0.7165 ,accuray= 0.520\n",
      "step: 405 ,batch_cost= 0.7107 ,accuray= 0.480\n",
      "step: 406 ,batch_cost= 0.6700 ,accuray= 0.560\n",
      "step: 407 ,batch_cost= 0.6520 ,accuray= 0.680\n",
      "step: 408 ,batch_cost= 0.6338 ,accuray= 0.660\n",
      "step: 409 ,batch_cost= 0.6639 ,accuray= 0.620\n",
      "step: 410 ,batch_cost= 0.7163 ,accuray= 0.480\n",
      "step: 411 ,batch_cost= 0.7422 ,accuray= 0.440\n",
      "step: 412 ,batch_cost= 0.6462 ,accuray= 0.680\n",
      "step: 413 ,batch_cost= 0.5892 ,accuray= 0.760\n",
      "step: 414 ,batch_cost= 0.7164 ,accuray= 0.520\n",
      "step: 415 ,batch_cost= 0.7107 ,accuray= 0.480\n",
      "step: 416 ,batch_cost= 0.6700 ,accuray= 0.560\n",
      "step: 417 ,batch_cost= 0.6519 ,accuray= 0.680\n",
      "step: 418 ,batch_cost= 0.6338 ,accuray= 0.660\n",
      "step: 419 ,batch_cost= 0.6639 ,accuray= 0.620\n",
      "step: 420 ,batch_cost= 0.7163 ,accuray= 0.480\n",
      "step: 421 ,batch_cost= 0.7422 ,accuray= 0.440\n",
      "step: 422 ,batch_cost= 0.6461 ,accuray= 0.680\n",
      "step: 423 ,batch_cost= 0.5892 ,accuray= 0.760\n",
      "step: 424 ,batch_cost= 0.7163 ,accuray= 0.520\n",
      "step: 425 ,batch_cost= 0.7107 ,accuray= 0.480\n",
      "step: 426 ,batch_cost= 0.6700 ,accuray= 0.560\n",
      "step: 427 ,batch_cost= 0.6519 ,accuray= 0.680\n",
      "step: 428 ,batch_cost= 0.6337 ,accuray= 0.660\n",
      "step: 429 ,batch_cost= 0.6639 ,accuray= 0.620\n",
      "step: 430 ,batch_cost= 0.7163 ,accuray= 0.480\n",
      "step: 431 ,batch_cost= 0.7422 ,accuray= 0.440\n",
      "step: 432 ,batch_cost= 0.6461 ,accuray= 0.680\n",
      "step: 433 ,batch_cost= 0.5892 ,accuray= 0.760\n",
      "step: 434 ,batch_cost= 0.7162 ,accuray= 0.520\n",
      "step: 435 ,batch_cost= 0.7107 ,accuray= 0.480\n",
      "step: 436 ,batch_cost= 0.6700 ,accuray= 0.560\n",
      "step: 437 ,batch_cost= 0.6519 ,accuray= 0.680\n",
      "step: 438 ,batch_cost= 0.6337 ,accuray= 0.660\n",
      "step: 439 ,batch_cost= 0.6639 ,accuray= 0.620\n",
      "step: 440 ,batch_cost= 0.7163 ,accuray= 0.480\n",
      "step: 441 ,batch_cost= 0.7421 ,accuray= 0.440\n",
      "step: 442 ,batch_cost= 0.6460 ,accuray= 0.680\n",
      "step: 443 ,batch_cost= 0.5892 ,accuray= 0.760\n",
      "step: 444 ,batch_cost= 0.7162 ,accuray= 0.520\n",
      "step: 445 ,batch_cost= 0.7107 ,accuray= 0.480\n",
      "step: 446 ,batch_cost= 0.6700 ,accuray= 0.560\n",
      "step: 447 ,batch_cost= 0.6518 ,accuray= 0.680\n",
      "step: 448 ,batch_cost= 0.6336 ,accuray= 0.660\n",
      "step: 449 ,batch_cost= 0.6639 ,accuray= 0.620\n",
      "step: 450 ,batch_cost= 0.7163 ,accuray= 0.480\n",
      "step: 451 ,batch_cost= 0.7421 ,accuray= 0.440\n",
      "step: 452 ,batch_cost= 0.6459 ,accuray= 0.680\n",
      "step: 453 ,batch_cost= 0.5892 ,accuray= 0.760\n",
      "step: 454 ,batch_cost= 0.7161 ,accuray= 0.520\n",
      "step: 455 ,batch_cost= 0.7107 ,accuray= 0.480\n",
      "step: 456 ,batch_cost= 0.6700 ,accuray= 0.560\n",
      "step: 457 ,batch_cost= 0.6518 ,accuray= 0.680\n",
      "step: 458 ,batch_cost= 0.6336 ,accuray= 0.660\n",
      "step: 459 ,batch_cost= 0.6638 ,accuray= 0.620\n",
      "step: 460 ,batch_cost= 0.7163 ,accuray= 0.480\n",
      "step: 461 ,batch_cost= 0.7420 ,accuray= 0.440\n",
      "step: 462 ,batch_cost= 0.6459 ,accuray= 0.680\n",
      "step: 463 ,batch_cost= 0.5892 ,accuray= 0.760\n",
      "step: 464 ,batch_cost= 0.7160 ,accuray= 0.520\n",
      "step: 465 ,batch_cost= 0.7107 ,accuray= 0.480\n",
      "step: 466 ,batch_cost= 0.6700 ,accuray= 0.560\n",
      "step: 467 ,batch_cost= 0.6518 ,accuray= 0.680\n",
      "step: 468 ,batch_cost= 0.6336 ,accuray= 0.660\n",
      "step: 469 ,batch_cost= 0.6638 ,accuray= 0.620\n",
      "step: 470 ,batch_cost= 0.7162 ,accuray= 0.480\n",
      "step: 471 ,batch_cost= 0.7420 ,accuray= 0.440\n",
      "step: 472 ,batch_cost= 0.6458 ,accuray= 0.680\n",
      "step: 473 ,batch_cost= 0.5892 ,accuray= 0.760\n",
      "step: 474 ,batch_cost= 0.7159 ,accuray= 0.520\n",
      "step: 475 ,batch_cost= 0.7107 ,accuray= 0.480\n",
      "step: 476 ,batch_cost= 0.6700 ,accuray= 0.560\n",
      "step: 477 ,batch_cost= 0.6518 ,accuray= 0.680\n",
      "step: 478 ,batch_cost= 0.6335 ,accuray= 0.660\n",
      "step: 479 ,batch_cost= 0.6638 ,accuray= 0.620\n",
      "step: 480 ,batch_cost= 0.7162 ,accuray= 0.480\n",
      "step: 481 ,batch_cost= 0.7419 ,accuray= 0.440\n",
      "step: 482 ,batch_cost= 0.6458 ,accuray= 0.680\n",
      "step: 483 ,batch_cost= 0.5892 ,accuray= 0.760\n",
      "step: 484 ,batch_cost= 0.7158 ,accuray= 0.520\n",
      "step: 485 ,batch_cost= 0.7108 ,accuray= 0.480\n",
      "step: 486 ,batch_cost= 0.6700 ,accuray= 0.560\n",
      "step: 487 ,batch_cost= 0.6518 ,accuray= 0.680\n",
      "step: 488 ,batch_cost= 0.6335 ,accuray= 0.660\n",
      "step: 489 ,batch_cost= 0.6638 ,accuray= 0.620\n",
      "step: 490 ,batch_cost= 0.7162 ,accuray= 0.480\n",
      "step: 491 ,batch_cost= 0.7419 ,accuray= 0.440\n",
      "step: 492 ,batch_cost= 0.6457 ,accuray= 0.680\n",
      "step: 493 ,batch_cost= 0.5892 ,accuray= 0.760\n",
      "step: 494 ,batch_cost= 0.7158 ,accuray= 0.520\n",
      "step: 495 ,batch_cost= 0.7108 ,accuray= 0.480\n",
      "step: 496 ,batch_cost= 0.6700 ,accuray= 0.560\n",
      "step: 497 ,batch_cost= 0.6517 ,accuray= 0.680\n",
      "step: 498 ,batch_cost= 0.6335 ,accuray= 0.660\n",
      "step: 499 ,batch_cost= 0.6638 ,accuray= 0.620\n",
      "step: 500 ,batch_cost= 0.7162 ,accuray= 0.480\n",
      "step: 501 ,batch_cost= 0.7418 ,accuray= 0.440\n",
      "step: 502 ,batch_cost= 0.6457 ,accuray= 0.680\n",
      "step: 503 ,batch_cost= 0.5892 ,accuray= 0.760\n",
      "step: 504 ,batch_cost= 0.7157 ,accuray= 0.520\n",
      "step: 505 ,batch_cost= 0.7108 ,accuray= 0.480\n",
      "step: 506 ,batch_cost= 0.6700 ,accuray= 0.560\n",
      "step: 507 ,batch_cost= 0.6517 ,accuray= 0.680\n",
      "step: 508 ,batch_cost= 0.6334 ,accuray= 0.660\n",
      "step: 509 ,batch_cost= 0.6638 ,accuray= 0.620\n",
      "step: 510 ,batch_cost= 0.7162 ,accuray= 0.480\n",
      "step: 511 ,batch_cost= 0.7418 ,accuray= 0.440\n",
      "step: 512 ,batch_cost= 0.6456 ,accuray= 0.680\n",
      "step: 513 ,batch_cost= 0.5892 ,accuray= 0.760\n",
      "step: 514 ,batch_cost= 0.7156 ,accuray= 0.520\n",
      "step: 515 ,batch_cost= 0.7108 ,accuray= 0.500\n",
      "step: 516 ,batch_cost= 0.6700 ,accuray= 0.560\n",
      "step: 517 ,batch_cost= 0.6517 ,accuray= 0.680\n",
      "step: 518 ,batch_cost= 0.6334 ,accuray= 0.660\n",
      "step: 519 ,batch_cost= 0.6638 ,accuray= 0.620\n",
      "step: 520 ,batch_cost= 0.7162 ,accuray= 0.480\n",
      "step: 521 ,batch_cost= 0.7417 ,accuray= 0.440\n",
      "step: 522 ,batch_cost= 0.6456 ,accuray= 0.680\n",
      "step: 523 ,batch_cost= 0.5892 ,accuray= 0.760\n",
      "step: 524 ,batch_cost= 0.7155 ,accuray= 0.520\n",
      "step: 525 ,batch_cost= 0.7108 ,accuray= 0.500\n",
      "step: 526 ,batch_cost= 0.6700 ,accuray= 0.560\n",
      "step: 527 ,batch_cost= 0.6517 ,accuray= 0.680\n",
      "step: 528 ,batch_cost= 0.6333 ,accuray= 0.660\n",
      "step: 529 ,batch_cost= 0.6638 ,accuray= 0.620\n",
      "step: 530 ,batch_cost= 0.7161 ,accuray= 0.480\n",
      "step: 531 ,batch_cost= 0.7417 ,accuray= 0.440\n",
      "step: 532 ,batch_cost= 0.6455 ,accuray= 0.680\n",
      "step: 533 ,batch_cost= 0.5892 ,accuray= 0.760\n",
      "step: 534 ,batch_cost= 0.7154 ,accuray= 0.520\n",
      "step: 535 ,batch_cost= 0.7108 ,accuray= 0.500\n",
      "step: 536 ,batch_cost= 0.6700 ,accuray= 0.560\n",
      "step: 537 ,batch_cost= 0.6517 ,accuray= 0.660\n",
      "step: 538 ,batch_cost= 0.6333 ,accuray= 0.660\n",
      "step: 539 ,batch_cost= 0.6638 ,accuray= 0.620\n",
      "step: 540 ,batch_cost= 0.7161 ,accuray= 0.480\n",
      "step: 541 ,batch_cost= 0.7416 ,accuray= 0.440\n",
      "step: 542 ,batch_cost= 0.6455 ,accuray= 0.680\n",
      "step: 543 ,batch_cost= 0.5893 ,accuray= 0.760\n",
      "step: 544 ,batch_cost= 0.7154 ,accuray= 0.520\n",
      "step: 545 ,batch_cost= 0.7108 ,accuray= 0.500\n",
      "step: 546 ,batch_cost= 0.6700 ,accuray= 0.560\n",
      "step: 547 ,batch_cost= 0.6517 ,accuray= 0.660\n",
      "step: 548 ,batch_cost= 0.6333 ,accuray= 0.660\n",
      "step: 549 ,batch_cost= 0.6638 ,accuray= 0.620\n",
      "step: 550 ,batch_cost= 0.7161 ,accuray= 0.480\n",
      "step: 551 ,batch_cost= 0.7416 ,accuray= 0.440\n",
      "step: 552 ,batch_cost= 0.6454 ,accuray= 0.680\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 553 ,batch_cost= 0.5893 ,accuray= 0.760\n",
      "step: 554 ,batch_cost= 0.7153 ,accuray= 0.520\n",
      "step: 555 ,batch_cost= 0.7108 ,accuray= 0.500\n",
      "step: 556 ,batch_cost= 0.6700 ,accuray= 0.560\n",
      "step: 557 ,batch_cost= 0.6516 ,accuray= 0.660\n",
      "step: 558 ,batch_cost= 0.6332 ,accuray= 0.660\n",
      "step: 559 ,batch_cost= 0.6638 ,accuray= 0.620\n",
      "step: 560 ,batch_cost= 0.7161 ,accuray= 0.480\n",
      "step: 561 ,batch_cost= 0.7415 ,accuray= 0.440\n",
      "step: 562 ,batch_cost= 0.6454 ,accuray= 0.680\n",
      "step: 563 ,batch_cost= 0.5893 ,accuray= 0.760\n",
      "step: 564 ,batch_cost= 0.7152 ,accuray= 0.520\n",
      "step: 565 ,batch_cost= 0.7108 ,accuray= 0.500\n",
      "step: 566 ,batch_cost= 0.6700 ,accuray= 0.560\n",
      "step: 567 ,batch_cost= 0.6516 ,accuray= 0.660\n",
      "step: 568 ,batch_cost= 0.6332 ,accuray= 0.660\n",
      "step: 569 ,batch_cost= 0.6637 ,accuray= 0.620\n",
      "step: 570 ,batch_cost= 0.7161 ,accuray= 0.480\n",
      "step: 571 ,batch_cost= 0.7415 ,accuray= 0.440\n",
      "step: 572 ,batch_cost= 0.6453 ,accuray= 0.680\n",
      "step: 573 ,batch_cost= 0.5893 ,accuray= 0.760\n",
      "step: 574 ,batch_cost= 0.7151 ,accuray= 0.520\n",
      "step: 575 ,batch_cost= 0.7108 ,accuray= 0.500\n",
      "step: 576 ,batch_cost= 0.6700 ,accuray= 0.560\n",
      "step: 577 ,batch_cost= 0.6516 ,accuray= 0.660\n",
      "step: 578 ,batch_cost= 0.6332 ,accuray= 0.660\n",
      "step: 579 ,batch_cost= 0.6637 ,accuray= 0.620\n",
      "step: 580 ,batch_cost= 0.7161 ,accuray= 0.480\n",
      "step: 581 ,batch_cost= 0.7414 ,accuray= 0.440\n",
      "step: 582 ,batch_cost= 0.6453 ,accuray= 0.680\n",
      "step: 583 ,batch_cost= 0.5893 ,accuray= 0.760\n",
      "step: 584 ,batch_cost= 0.7150 ,accuray= 0.520\n",
      "step: 585 ,batch_cost= 0.7108 ,accuray= 0.500\n",
      "step: 586 ,batch_cost= 0.6700 ,accuray= 0.560\n",
      "step: 587 ,batch_cost= 0.6516 ,accuray= 0.660\n",
      "step: 588 ,batch_cost= 0.6331 ,accuray= 0.660\n",
      "step: 589 ,batch_cost= 0.6637 ,accuray= 0.620\n",
      "step: 590 ,batch_cost= 0.7160 ,accuray= 0.480\n",
      "step: 591 ,batch_cost= 0.7413 ,accuray= 0.440\n",
      "step: 592 ,batch_cost= 0.6452 ,accuray= 0.680\n",
      "step: 593 ,batch_cost= 0.5893 ,accuray= 0.760\n",
      "step: 594 ,batch_cost= 0.7150 ,accuray= 0.520\n",
      "step: 595 ,batch_cost= 0.7108 ,accuray= 0.500\n",
      "step: 596 ,batch_cost= 0.6700 ,accuray= 0.560\n",
      "step: 597 ,batch_cost= 0.6516 ,accuray= 0.660\n",
      "step: 598 ,batch_cost= 0.6331 ,accuray= 0.660\n",
      "step: 599 ,batch_cost= 0.6637 ,accuray= 0.620\n",
      "step: 601 ,batch_cost= 0.7413 ,accuray= 0.440\n",
      "step: 602 ,batch_cost= 0.6452 ,accuray= 0.680\n",
      "step: 603 ,batch_cost= 0.5893 ,accuray= 0.760\n",
      "step: 604 ,batch_cost= 0.7149 ,accuray= 0.520\n",
      "step: 605 ,batch_cost= 0.7108 ,accuray= 0.500\n",
      "step: 606 ,batch_cost= 0.6700 ,accuray= 0.560\n",
      "step: 607 ,batch_cost= 0.6515 ,accuray= 0.660\n",
      "step: 608 ,batch_cost= 0.6331 ,accuray= 0.660\n",
      "step: 609 ,batch_cost= 0.6637 ,accuray= 0.620\n",
      "step: 610 ,batch_cost= 0.7160 ,accuray= 0.480\n",
      "step: 611 ,batch_cost= 0.7412 ,accuray= 0.440\n",
      "step: 612 ,batch_cost= 0.6451 ,accuray= 0.680\n",
      "step: 613 ,batch_cost= 0.5893 ,accuray= 0.760\n",
      "step: 614 ,batch_cost= 0.7148 ,accuray= 0.520\n",
      "step: 615 ,batch_cost= 0.7108 ,accuray= 0.500\n",
      "step: 616 ,batch_cost= 0.6700 ,accuray= 0.560\n",
      "step: 617 ,batch_cost= 0.6515 ,accuray= 0.660\n",
      "step: 618 ,batch_cost= 0.6330 ,accuray= 0.660\n",
      "step: 619 ,batch_cost= 0.6637 ,accuray= 0.620\n",
      "step: 620 ,batch_cost= 0.7160 ,accuray= 0.480\n",
      "step: 621 ,batch_cost= 0.7412 ,accuray= 0.440\n",
      "step: 622 ,batch_cost= 0.6451 ,accuray= 0.680\n",
      "step: 623 ,batch_cost= 0.5893 ,accuray= 0.760\n",
      "step: 624 ,batch_cost= 0.7147 ,accuray= 0.520\n",
      "step: 625 ,batch_cost= 0.7108 ,accuray= 0.500\n",
      "step: 626 ,batch_cost= 0.6700 ,accuray= 0.560\n",
      "step: 627 ,batch_cost= 0.6515 ,accuray= 0.660\n",
      "step: 628 ,batch_cost= 0.6330 ,accuray= 0.660\n",
      "step: 629 ,batch_cost= 0.6637 ,accuray= 0.620\n",
      "step: 630 ,batch_cost= 0.7160 ,accuray= 0.480\n",
      "step: 631 ,batch_cost= 0.7411 ,accuray= 0.440\n",
      "step: 632 ,batch_cost= 0.6450 ,accuray= 0.680\n",
      "step: 633 ,batch_cost= 0.5893 ,accuray= 0.760\n",
      "step: 634 ,batch_cost= 0.7147 ,accuray= 0.520\n",
      "step: 635 ,batch_cost= 0.7108 ,accuray= 0.500\n",
      "step: 636 ,batch_cost= 0.6700 ,accuray= 0.560\n",
      "step: 637 ,batch_cost= 0.6515 ,accuray= 0.660\n",
      "step: 638 ,batch_cost= 0.6330 ,accuray= 0.660\n",
      "step: 639 ,batch_cost= 0.6637 ,accuray= 0.620\n",
      "step: 640 ,batch_cost= 0.7160 ,accuray= 0.480\n",
      "step: 641 ,batch_cost= 0.7411 ,accuray= 0.440\n",
      "step: 642 ,batch_cost= 0.6450 ,accuray= 0.680\n",
      "step: 643 ,batch_cost= 0.5893 ,accuray= 0.760\n",
      "step: 644 ,batch_cost= 0.7146 ,accuray= 0.520\n",
      "step: 645 ,batch_cost= 0.7108 ,accuray= 0.500\n",
      "step: 646 ,batch_cost= 0.6700 ,accuray= 0.560\n",
      "step: 647 ,batch_cost= 0.6515 ,accuray= 0.660\n",
      "step: 648 ,batch_cost= 0.6329 ,accuray= 0.660\n",
      "step: 649 ,batch_cost= 0.6637 ,accuray= 0.620\n",
      "step: 650 ,batch_cost= 0.7159 ,accuray= 0.480\n",
      "step: 651 ,batch_cost= 0.7410 ,accuray= 0.440\n",
      "step: 652 ,batch_cost= 0.6449 ,accuray= 0.680\n",
      "step: 653 ,batch_cost= 0.5893 ,accuray= 0.760\n",
      "step: 654 ,batch_cost= 0.7145 ,accuray= 0.520\n",
      "step: 655 ,batch_cost= 0.7108 ,accuray= 0.500\n",
      "step: 656 ,batch_cost= 0.6700 ,accuray= 0.560\n",
      "step: 657 ,batch_cost= 0.6515 ,accuray= 0.660\n",
      "step: 658 ,batch_cost= 0.6329 ,accuray= 0.660\n",
      "step: 659 ,batch_cost= 0.6637 ,accuray= 0.620\n",
      "step: 660 ,batch_cost= 0.7159 ,accuray= 0.480\n",
      "step: 661 ,batch_cost= 0.7410 ,accuray= 0.440\n",
      "step: 662 ,batch_cost= 0.6449 ,accuray= 0.680\n",
      "step: 663 ,batch_cost= 0.5893 ,accuray= 0.760\n",
      "step: 664 ,batch_cost= 0.7144 ,accuray= 0.520\n",
      "step: 665 ,batch_cost= 0.7108 ,accuray= 0.500\n",
      "step: 666 ,batch_cost= 0.6700 ,accuray= 0.560\n",
      "step: 667 ,batch_cost= 0.6515 ,accuray= 0.660\n",
      "step: 668 ,batch_cost= 0.6329 ,accuray= 0.660\n",
      "step: 669 ,batch_cost= 0.6637 ,accuray= 0.620\n",
      "step: 670 ,batch_cost= 0.7159 ,accuray= 0.480\n",
      "step: 671 ,batch_cost= 0.7409 ,accuray= 0.440\n",
      "step: 672 ,batch_cost= 0.6448 ,accuray= 0.680\n",
      "step: 673 ,batch_cost= 0.5894 ,accuray= 0.760\n",
      "step: 674 ,batch_cost= 0.7143 ,accuray= 0.520\n",
      "step: 675 ,batch_cost= 0.7107 ,accuray= 0.500\n",
      "step: 676 ,batch_cost= 0.6700 ,accuray= 0.560\n",
      "step: 677 ,batch_cost= 0.6514 ,accuray= 0.660\n",
      "step: 678 ,batch_cost= 0.6328 ,accuray= 0.660\n",
      "step: 679 ,batch_cost= 0.6636 ,accuray= 0.620\n",
      "step: 680 ,batch_cost= 0.7159 ,accuray= 0.480\n",
      "step: 681 ,batch_cost= 0.7409 ,accuray= 0.440\n",
      "step: 682 ,batch_cost= 0.6448 ,accuray= 0.680\n",
      "step: 683 ,batch_cost= 0.5894 ,accuray= 0.760\n",
      "step: 684 ,batch_cost= 0.7143 ,accuray= 0.520\n",
      "step: 685 ,batch_cost= 0.7107 ,accuray= 0.500\n",
      "step: 686 ,batch_cost= 0.6700 ,accuray= 0.560\n",
      "step: 687 ,batch_cost= 0.6514 ,accuray= 0.660\n",
      "step: 688 ,batch_cost= 0.6328 ,accuray= 0.660\n",
      "step: 689 ,batch_cost= 0.6636 ,accuray= 0.620\n",
      "step: 690 ,batch_cost= 0.7159 ,accuray= 0.480\n",
      "step: 691 ,batch_cost= 0.7408 ,accuray= 0.440\n",
      "step: 692 ,batch_cost= 0.6447 ,accuray= 0.680\n",
      "step: 693 ,batch_cost= 0.5894 ,accuray= 0.760\n",
      "step: 694 ,batch_cost= 0.7142 ,accuray= 0.520\n",
      "step: 695 ,batch_cost= 0.7107 ,accuray= 0.500\n",
      "step: 696 ,batch_cost= 0.6700 ,accuray= 0.560\n",
      "step: 697 ,batch_cost= 0.6514 ,accuray= 0.660\n",
      "step: 698 ,batch_cost= 0.6328 ,accuray= 0.660\n",
      "step: 699 ,batch_cost= 0.6636 ,accuray= 0.620\n",
      "step: 700 ,batch_cost= 0.7158 ,accuray= 0.480\n",
      "step: 701 ,batch_cost= 0.7408 ,accuray= 0.440\n",
      "step: 702 ,batch_cost= 0.6447 ,accuray= 0.680\n",
      "step: 703 ,batch_cost= 0.5894 ,accuray= 0.760\n",
      "step: 704 ,batch_cost= 0.7141 ,accuray= 0.520\n",
      "step: 705 ,batch_cost= 0.7107 ,accuray= 0.500\n",
      "step: 706 ,batch_cost= 0.6700 ,accuray= 0.560\n",
      "step: 707 ,batch_cost= 0.6514 ,accuray= 0.640\n",
      "step: 708 ,batch_cost= 0.6327 ,accuray= 0.660\n",
      "step: 709 ,batch_cost= 0.6636 ,accuray= 0.620\n",
      "step: 710 ,batch_cost= 0.7158 ,accuray= 0.480\n",
      "step: 711 ,batch_cost= 0.7407 ,accuray= 0.440\n",
      "step: 712 ,batch_cost= 0.6447 ,accuray= 0.680\n",
      "step: 713 ,batch_cost= 0.5894 ,accuray= 0.760\n",
      "step: 714 ,batch_cost= 0.7140 ,accuray= 0.520\n",
      "step: 715 ,batch_cost= 0.7107 ,accuray= 0.500\n",
      "step: 716 ,batch_cost= 0.6700 ,accuray= 0.560\n",
      "step: 717 ,batch_cost= 0.6514 ,accuray= 0.640\n",
      "step: 718 ,batch_cost= 0.6327 ,accuray= 0.660\n",
      "step: 719 ,batch_cost= 0.6636 ,accuray= 0.620\n",
      "step: 720 ,batch_cost= 0.7158 ,accuray= 0.480\n",
      "step: 721 ,batch_cost= 0.7407 ,accuray= 0.440\n",
      "step: 722 ,batch_cost= 0.6446 ,accuray= 0.680\n",
      "step: 723 ,batch_cost= 0.5894 ,accuray= 0.760\n",
      "step: 724 ,batch_cost= 0.7139 ,accuray= 0.520\n",
      "step: 725 ,batch_cost= 0.7107 ,accuray= 0.500\n",
      "step: 726 ,batch_cost= 0.6700 ,accuray= 0.560\n",
      "step: 727 ,batch_cost= 0.6514 ,accuray= 0.640\n",
      "step: 728 ,batch_cost= 0.6327 ,accuray= 0.660\n",
      "step: 729 ,batch_cost= 0.6636 ,accuray= 0.620\n",
      "step: 730 ,batch_cost= 0.7158 ,accuray= 0.480\n",
      "step: 731 ,batch_cost= 0.7406 ,accuray= 0.440\n",
      "step: 732 ,batch_cost= 0.6446 ,accuray= 0.680\n",
      "step: 733 ,batch_cost= 0.5894 ,accuray= 0.760\n",
      "step: 734 ,batch_cost= 0.7139 ,accuray= 0.520\n",
      "step: 735 ,batch_cost= 0.7107 ,accuray= 0.500\n",
      "step: 736 ,batch_cost= 0.6700 ,accuray= 0.560\n",
      "step: 737 ,batch_cost= 0.6513 ,accuray= 0.640\n",
      "step: 738 ,batch_cost= 0.6326 ,accuray= 0.660\n",
      "step: 739 ,batch_cost= 0.6636 ,accuray= 0.620\n",
      "step: 740 ,batch_cost= 0.7158 ,accuray= 0.480\n",
      "step: 741 ,batch_cost= 0.7405 ,accuray= 0.440\n",
      "step: 742 ,batch_cost= 0.6445 ,accuray= 0.680\n",
      "step: 743 ,batch_cost= 0.5894 ,accuray= 0.760\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 744 ,batch_cost= 0.7138 ,accuray= 0.520\n",
      "step: 745 ,batch_cost= 0.7107 ,accuray= 0.500\n",
      "step: 746 ,batch_cost= 0.6700 ,accuray= 0.560\n",
      "step: 747 ,batch_cost= 0.6513 ,accuray= 0.640\n",
      "step: 748 ,batch_cost= 0.6326 ,accuray= 0.660\n",
      "step: 749 ,batch_cost= 0.6636 ,accuray= 0.620\n",
      "step: 750 ,batch_cost= 0.7158 ,accuray= 0.480\n",
      "step: 751 ,batch_cost= 0.7405 ,accuray= 0.440\n",
      "step: 752 ,batch_cost= 0.6445 ,accuray= 0.680\n",
      "step: 753 ,batch_cost= 0.5894 ,accuray= 0.760\n",
      "step: 754 ,batch_cost= 0.7137 ,accuray= 0.520\n",
      "step: 755 ,batch_cost= 0.7107 ,accuray= 0.500\n",
      "step: 756 ,batch_cost= 0.6700 ,accuray= 0.560\n",
      "step: 757 ,batch_cost= 0.6513 ,accuray= 0.640\n",
      "step: 758 ,batch_cost= 0.6326 ,accuray= 0.660\n",
      "step: 759 ,batch_cost= 0.6636 ,accuray= 0.620\n",
      "step: 760 ,batch_cost= 0.7157 ,accuray= 0.480\n",
      "step: 761 ,batch_cost= 0.7404 ,accuray= 0.440\n",
      "step: 762 ,batch_cost= 0.6444 ,accuray= 0.680\n",
      "step: 763 ,batch_cost= 0.5894 ,accuray= 0.760\n",
      "step: 764 ,batch_cost= 0.7136 ,accuray= 0.520\n",
      "step: 765 ,batch_cost= 0.7107 ,accuray= 0.500\n",
      "step: 766 ,batch_cost= 0.6700 ,accuray= 0.560\n",
      "step: 767 ,batch_cost= 0.6513 ,accuray= 0.640\n",
      "step: 768 ,batch_cost= 0.6325 ,accuray= 0.660\n",
      "step: 769 ,batch_cost= 0.6636 ,accuray= 0.620\n",
      "step: 770 ,batch_cost= 0.7157 ,accuray= 0.480\n",
      "step: 771 ,batch_cost= 0.7404 ,accuray= 0.440\n",
      "step: 772 ,batch_cost= 0.6444 ,accuray= 0.680\n",
      "step: 773 ,batch_cost= 0.5894 ,accuray= 0.760\n",
      "step: 774 ,batch_cost= 0.7136 ,accuray= 0.520\n",
      "step: 775 ,batch_cost= 0.7107 ,accuray= 0.500\n",
      "step: 776 ,batch_cost= 0.6700 ,accuray= 0.560\n",
      "step: 777 ,batch_cost= 0.6513 ,accuray= 0.640\n",
      "step: 778 ,batch_cost= 0.6325 ,accuray= 0.660\n",
      "step: 779 ,batch_cost= 0.6636 ,accuray= 0.620\n",
      "step: 780 ,batch_cost= 0.7157 ,accuray= 0.480\n",
      "step: 781 ,batch_cost= 0.7403 ,accuray= 0.440\n",
      "step: 782 ,batch_cost= 0.6443 ,accuray= 0.680\n",
      "step: 783 ,batch_cost= 0.5894 ,accuray= 0.760\n",
      "step: 784 ,batch_cost= 0.7135 ,accuray= 0.520\n",
      "step: 785 ,batch_cost= 0.7107 ,accuray= 0.500\n",
      "step: 786 ,batch_cost= 0.6700 ,accuray= 0.560\n",
      "step: 787 ,batch_cost= 0.6513 ,accuray= 0.640\n",
      "step: 788 ,batch_cost= 0.6325 ,accuray= 0.660\n",
      "step: 789 ,batch_cost= 0.6636 ,accuray= 0.620\n",
      "step: 790 ,batch_cost= 0.7157 ,accuray= 0.480\n",
      "step: 791 ,batch_cost= 0.7403 ,accuray= 0.440\n",
      "step: 792 ,batch_cost= 0.6443 ,accuray= 0.680\n",
      "step: 793 ,batch_cost= 0.5895 ,accuray= 0.760\n",
      "step: 794 ,batch_cost= 0.7134 ,accuray= 0.520\n",
      "step: 795 ,batch_cost= 0.7107 ,accuray= 0.500\n",
      "step: 796 ,batch_cost= 0.6700 ,accuray= 0.560\n",
      "step: 797 ,batch_cost= 0.6512 ,accuray= 0.640\n",
      "step: 798 ,batch_cost= 0.6325 ,accuray= 0.660\n",
      "step: 799 ,batch_cost= 0.6635 ,accuray= 0.620\n",
      "step: 801 ,batch_cost= 0.7402 ,accuray= 0.440\n",
      "step: 802 ,batch_cost= 0.6442 ,accuray= 0.680\n",
      "step: 803 ,batch_cost= 0.5895 ,accuray= 0.760\n",
      "step: 804 ,batch_cost= 0.7133 ,accuray= 0.520\n",
      "step: 805 ,batch_cost= 0.7107 ,accuray= 0.500\n",
      "step: 806 ,batch_cost= 0.6700 ,accuray= 0.560\n",
      "step: 807 ,batch_cost= 0.6512 ,accuray= 0.640\n",
      "step: 808 ,batch_cost= 0.6324 ,accuray= 0.660\n",
      "step: 809 ,batch_cost= 0.6635 ,accuray= 0.620\n",
      "step: 810 ,batch_cost= 0.7156 ,accuray= 0.480\n",
      "step: 811 ,batch_cost= 0.7402 ,accuray= 0.440\n",
      "step: 812 ,batch_cost= 0.6442 ,accuray= 0.680\n",
      "step: 813 ,batch_cost= 0.5895 ,accuray= 0.760\n",
      "step: 814 ,batch_cost= 0.7132 ,accuray= 0.520\n",
      "step: 815 ,batch_cost= 0.7107 ,accuray= 0.500\n",
      "step: 816 ,batch_cost= 0.6700 ,accuray= 0.560\n",
      "step: 817 ,batch_cost= 0.6512 ,accuray= 0.640\n",
      "step: 818 ,batch_cost= 0.6324 ,accuray= 0.660\n",
      "step: 819 ,batch_cost= 0.6635 ,accuray= 0.620\n",
      "step: 820 ,batch_cost= 0.7156 ,accuray= 0.480\n",
      "step: 821 ,batch_cost= 0.7401 ,accuray= 0.440\n",
      "step: 822 ,batch_cost= 0.6441 ,accuray= 0.680\n",
      "step: 823 ,batch_cost= 0.5895 ,accuray= 0.760\n",
      "step: 824 ,batch_cost= 0.7132 ,accuray= 0.520\n",
      "step: 825 ,batch_cost= 0.7107 ,accuray= 0.500\n",
      "step: 826 ,batch_cost= 0.6700 ,accuray= 0.560\n",
      "step: 827 ,batch_cost= 0.6512 ,accuray= 0.640\n",
      "step: 828 ,batch_cost= 0.6324 ,accuray= 0.660\n",
      "step: 829 ,batch_cost= 0.6635 ,accuray= 0.620\n",
      "step: 830 ,batch_cost= 0.7156 ,accuray= 0.480\n",
      "step: 831 ,batch_cost= 0.7401 ,accuray= 0.440\n",
      "step: 832 ,batch_cost= 0.6441 ,accuray= 0.680\n",
      "step: 833 ,batch_cost= 0.5895 ,accuray= 0.760\n",
      "step: 834 ,batch_cost= 0.7131 ,accuray= 0.520\n",
      "step: 835 ,batch_cost= 0.7107 ,accuray= 0.500\n",
      "step: 836 ,batch_cost= 0.6700 ,accuray= 0.560\n",
      "step: 837 ,batch_cost= 0.6512 ,accuray= 0.640\n",
      "step: 838 ,batch_cost= 0.6323 ,accuray= 0.660\n",
      "step: 839 ,batch_cost= 0.6635 ,accuray= 0.620\n",
      "step: 840 ,batch_cost= 0.7156 ,accuray= 0.480\n",
      "step: 841 ,batch_cost= 0.7400 ,accuray= 0.440\n",
      "step: 842 ,batch_cost= 0.6440 ,accuray= 0.680\n",
      "step: 843 ,batch_cost= 0.5895 ,accuray= 0.760\n",
      "step: 844 ,batch_cost= 0.7130 ,accuray= 0.520\n",
      "step: 845 ,batch_cost= 0.7107 ,accuray= 0.500\n",
      "step: 846 ,batch_cost= 0.6700 ,accuray= 0.560\n",
      "step: 847 ,batch_cost= 0.6512 ,accuray= 0.640\n",
      "step: 848 ,batch_cost= 0.6323 ,accuray= 0.660\n",
      "step: 849 ,batch_cost= 0.6635 ,accuray= 0.620\n",
      "step: 850 ,batch_cost= 0.7156 ,accuray= 0.480\n",
      "step: 851 ,batch_cost= 0.7400 ,accuray= 0.440\n",
      "step: 852 ,batch_cost= 0.6440 ,accuray= 0.680\n",
      "step: 853 ,batch_cost= 0.5895 ,accuray= 0.760\n",
      "step: 854 ,batch_cost= 0.7129 ,accuray= 0.520\n",
      "step: 855 ,batch_cost= 0.7107 ,accuray= 0.500\n",
      "step: 856 ,batch_cost= 0.6700 ,accuray= 0.560\n",
      "step: 857 ,batch_cost= 0.6512 ,accuray= 0.640\n",
      "step: 858 ,batch_cost= 0.6323 ,accuray= 0.660\n",
      "step: 859 ,batch_cost= 0.6635 ,accuray= 0.620\n",
      "step: 860 ,batch_cost= 0.7156 ,accuray= 0.480\n",
      "step: 861 ,batch_cost= 0.7399 ,accuray= 0.440\n",
      "step: 862 ,batch_cost= 0.6439 ,accuray= 0.680\n",
      "step: 863 ,batch_cost= 0.5895 ,accuray= 0.760\n",
      "step: 864 ,batch_cost= 0.7129 ,accuray= 0.520\n",
      "step: 865 ,batch_cost= 0.7107 ,accuray= 0.500\n",
      "step: 866 ,batch_cost= 0.6700 ,accuray= 0.560\n",
      "step: 867 ,batch_cost= 0.6511 ,accuray= 0.640\n",
      "step: 868 ,batch_cost= 0.6322 ,accuray= 0.660\n",
      "step: 869 ,batch_cost= 0.6635 ,accuray= 0.620\n",
      "step: 870 ,batch_cost= 0.7155 ,accuray= 0.480\n",
      "step: 871 ,batch_cost= 0.7399 ,accuray= 0.440\n",
      "step: 872 ,batch_cost= 0.6439 ,accuray= 0.680\n",
      "step: 873 ,batch_cost= 0.5895 ,accuray= 0.760\n",
      "step: 874 ,batch_cost= 0.7128 ,accuray= 0.520\n",
      "step: 875 ,batch_cost= 0.7107 ,accuray= 0.500\n",
      "step: 876 ,batch_cost= 0.6700 ,accuray= 0.560\n",
      "step: 877 ,batch_cost= 0.6511 ,accuray= 0.640\n",
      "step: 878 ,batch_cost= 0.6322 ,accuray= 0.660\n",
      "step: 879 ,batch_cost= 0.6635 ,accuray= 0.620\n",
      "step: 880 ,batch_cost= 0.7155 ,accuray= 0.480\n",
      "step: 881 ,batch_cost= 0.7398 ,accuray= 0.440\n",
      "step: 882 ,batch_cost= 0.6438 ,accuray= 0.680\n",
      "step: 883 ,batch_cost= 0.5895 ,accuray= 0.760\n",
      "step: 884 ,batch_cost= 0.7127 ,accuray= 0.520\n",
      "step: 885 ,batch_cost= 0.7107 ,accuray= 0.500\n",
      "step: 886 ,batch_cost= 0.6700 ,accuray= 0.560\n",
      "step: 887 ,batch_cost= 0.6511 ,accuray= 0.640\n",
      "step: 888 ,batch_cost= 0.6322 ,accuray= 0.660\n",
      "step: 889 ,batch_cost= 0.6635 ,accuray= 0.620\n",
      "step: 890 ,batch_cost= 0.7155 ,accuray= 0.480\n",
      "step: 891 ,batch_cost= 0.7398 ,accuray= 0.440\n",
      "step: 892 ,batch_cost= 0.6438 ,accuray= 0.680\n",
      "step: 893 ,batch_cost= 0.5895 ,accuray= 0.760\n",
      "step: 894 ,batch_cost= 0.7126 ,accuray= 0.520\n",
      "step: 895 ,batch_cost= 0.7107 ,accuray= 0.500\n",
      "step: 896 ,batch_cost= 0.6700 ,accuray= 0.560\n",
      "step: 897 ,batch_cost= 0.6511 ,accuray= 0.640\n",
      "step: 898 ,batch_cost= 0.6321 ,accuray= 0.660\n",
      "step: 899 ,batch_cost= 0.6635 ,accuray= 0.620\n",
      "step: 900 ,batch_cost= 0.7155 ,accuray= 0.480\n",
      "step: 901 ,batch_cost= 0.7397 ,accuray= 0.440\n",
      "step: 902 ,batch_cost= 0.6437 ,accuray= 0.680\n",
      "step: 903 ,batch_cost= 0.5895 ,accuray= 0.760\n",
      "step: 904 ,batch_cost= 0.7125 ,accuray= 0.520\n",
      "step: 905 ,batch_cost= 0.7107 ,accuray= 0.500\n",
      "step: 906 ,batch_cost= 0.6700 ,accuray= 0.560\n",
      "step: 907 ,batch_cost= 0.6511 ,accuray= 0.640\n",
      "step: 908 ,batch_cost= 0.6321 ,accuray= 0.660\n",
      "step: 909 ,batch_cost= 0.6634 ,accuray= 0.620\n",
      "step: 910 ,batch_cost= 0.7155 ,accuray= 0.480\n",
      "step: 911 ,batch_cost= 0.7397 ,accuray= 0.440\n",
      "step: 912 ,batch_cost= 0.6437 ,accuray= 0.680\n",
      "step: 913 ,batch_cost= 0.5895 ,accuray= 0.760\n",
      "step: 914 ,batch_cost= 0.7125 ,accuray= 0.520\n",
      "step: 915 ,batch_cost= 0.7107 ,accuray= 0.500\n",
      "step: 916 ,batch_cost= 0.6700 ,accuray= 0.560\n",
      "step: 917 ,batch_cost= 0.6511 ,accuray= 0.640\n",
      "step: 918 ,batch_cost= 0.6321 ,accuray= 0.660\n",
      "step: 919 ,batch_cost= 0.6634 ,accuray= 0.620\n",
      "step: 920 ,batch_cost= 0.7155 ,accuray= 0.480\n",
      "step: 921 ,batch_cost= 0.7396 ,accuray= 0.440\n",
      "step: 922 ,batch_cost= 0.6436 ,accuray= 0.680\n",
      "step: 923 ,batch_cost= 0.5896 ,accuray= 0.760\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 924 ,batch_cost= 0.7124 ,accuray= 0.520\n",
      "step: 925 ,batch_cost= 0.7107 ,accuray= 0.500\n",
      "step: 926 ,batch_cost= 0.6700 ,accuray= 0.560\n",
      "step: 927 ,batch_cost= 0.6510 ,accuray= 0.640\n",
      "step: 928 ,batch_cost= 0.6320 ,accuray= 0.660\n",
      "step: 929 ,batch_cost= 0.6634 ,accuray= 0.620\n",
      "step: 930 ,batch_cost= 0.7154 ,accuray= 0.480\n",
      "step: 931 ,batch_cost= 0.7396 ,accuray= 0.440\n",
      "step: 932 ,batch_cost= 0.6436 ,accuray= 0.680\n",
      "step: 933 ,batch_cost= 0.5896 ,accuray= 0.760\n",
      "step: 934 ,batch_cost= 0.7123 ,accuray= 0.520\n",
      "step: 935 ,batch_cost= 0.7107 ,accuray= 0.500\n",
      "step: 936 ,batch_cost= 0.6700 ,accuray= 0.560\n",
      "step: 937 ,batch_cost= 0.6510 ,accuray= 0.640\n",
      "step: 938 ,batch_cost= 0.6320 ,accuray= 0.660\n",
      "step: 939 ,batch_cost= 0.6634 ,accuray= 0.620\n",
      "step: 940 ,batch_cost= 0.7154 ,accuray= 0.480\n",
      "step: 941 ,batch_cost= 0.7395 ,accuray= 0.440\n",
      "step: 942 ,batch_cost= 0.6436 ,accuray= 0.680\n",
      "step: 943 ,batch_cost= 0.5896 ,accuray= 0.760\n",
      "step: 944 ,batch_cost= 0.7122 ,accuray= 0.520\n",
      "step: 945 ,batch_cost= 0.7107 ,accuray= 0.500\n",
      "step: 946 ,batch_cost= 0.6700 ,accuray= 0.560\n",
      "step: 947 ,batch_cost= 0.6510 ,accuray= 0.640\n",
      "step: 948 ,batch_cost= 0.6320 ,accuray= 0.660\n",
      "step: 949 ,batch_cost= 0.6634 ,accuray= 0.620\n",
      "step: 950 ,batch_cost= 0.7154 ,accuray= 0.480\n",
      "step: 951 ,batch_cost= 0.7395 ,accuray= 0.440\n",
      "step: 952 ,batch_cost= 0.6435 ,accuray= 0.680\n",
      "step: 953 ,batch_cost= 0.5896 ,accuray= 0.760\n",
      "step: 954 ,batch_cost= 0.7122 ,accuray= 0.520\n",
      "step: 955 ,batch_cost= 0.7107 ,accuray= 0.500\n",
      "step: 956 ,batch_cost= 0.6700 ,accuray= 0.560\n",
      "step: 957 ,batch_cost= 0.6510 ,accuray= 0.640\n",
      "step: 958 ,batch_cost= 0.6319 ,accuray= 0.660\n",
      "step: 959 ,batch_cost= 0.6634 ,accuray= 0.620\n",
      "step: 960 ,batch_cost= 0.7154 ,accuray= 0.480\n",
      "step: 961 ,batch_cost= 0.7394 ,accuray= 0.440\n",
      "step: 962 ,batch_cost= 0.6435 ,accuray= 0.680\n",
      "step: 963 ,batch_cost= 0.5896 ,accuray= 0.760\n",
      "step: 964 ,batch_cost= 0.7121 ,accuray= 0.520\n",
      "step: 965 ,batch_cost= 0.7107 ,accuray= 0.500\n",
      "step: 966 ,batch_cost= 0.6700 ,accuray= 0.560\n",
      "step: 967 ,batch_cost= 0.6510 ,accuray= 0.640\n",
      "step: 968 ,batch_cost= 0.6319 ,accuray= 0.660\n",
      "step: 969 ,batch_cost= 0.6634 ,accuray= 0.620\n",
      "step: 970 ,batch_cost= 0.7154 ,accuray= 0.480\n",
      "step: 971 ,batch_cost= 0.7393 ,accuray= 0.440\n",
      "step: 972 ,batch_cost= 0.6434 ,accuray= 0.680\n",
      "step: 973 ,batch_cost= 0.5896 ,accuray= 0.760\n",
      "step: 974 ,batch_cost= 0.7120 ,accuray= 0.520\n",
      "step: 975 ,batch_cost= 0.7107 ,accuray= 0.500\n",
      "step: 976 ,batch_cost= 0.6700 ,accuray= 0.560\n",
      "step: 977 ,batch_cost= 0.6510 ,accuray= 0.640\n",
      "step: 978 ,batch_cost= 0.6319 ,accuray= 0.660\n",
      "step: 979 ,batch_cost= 0.6634 ,accuray= 0.620\n",
      "step: 980 ,batch_cost= 0.7153 ,accuray= 0.480\n",
      "step: 981 ,batch_cost= 0.7393 ,accuray= 0.440\n",
      "step: 982 ,batch_cost= 0.6434 ,accuray= 0.680\n",
      "step: 983 ,batch_cost= 0.5896 ,accuray= 0.760\n",
      "step: 984 ,batch_cost= 0.7119 ,accuray= 0.520\n",
      "step: 985 ,batch_cost= 0.7107 ,accuray= 0.500\n",
      "step: 986 ,batch_cost= 0.6700 ,accuray= 0.560\n",
      "step: 987 ,batch_cost= 0.6510 ,accuray= 0.640\n",
      "step: 988 ,batch_cost= 0.6318 ,accuray= 0.660\n",
      "step: 989 ,batch_cost= 0.6634 ,accuray= 0.620\n",
      "step: 990 ,batch_cost= 0.7153 ,accuray= 0.480\n",
      "step: 991 ,batch_cost= 0.7392 ,accuray= 0.440\n",
      "step: 992 ,batch_cost= 0.6433 ,accuray= 0.680\n",
      "step: 993 ,batch_cost= 0.5896 ,accuray= 0.760\n",
      "step: 994 ,batch_cost= 0.7119 ,accuray= 0.520\n",
      "step: 995 ,batch_cost= 0.7107 ,accuray= 0.500\n",
      "step: 996 ,batch_cost= 0.6700 ,accuray= 0.560\n",
      "step: 997 ,batch_cost= 0.6509 ,accuray= 0.640\n",
      "step: 998 ,batch_cost= 0.6318 ,accuray= 0.660\n",
      "step: 999 ,batch_cost= 0.6634 ,accuray= 0.620\n"
     ]
    }
   ],
   "source": [
    "class basic_LSTM(object):\n",
    "    def __init__(self, sess, time_step, input_dim, hidden_size, class_num, batch_size):\n",
    "        self.time_step = time_step\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.class_num = class_num\n",
    "        self.batch_size = batch_size\n",
    "        self.rnn_cell = tf.contrib.rnn.BasicLSTMCell(hidden_size)\n",
    "        \n",
    "        self.input_x = tf.placeholder(dtype = tf.float32, shape = [None,time_step,input_dim])\n",
    "        self.label_y = tf.placeholder(dtype = tf.float32, shape = [None, class_num])\n",
    "        \n",
    "        state = self.rnn_cell.zero_state(batch_size = self.batch_size, dtype = tf.float32)\n",
    "        \n",
    "        self.output_list = []\n",
    "        for each_time_step in tf.unstack(self.input_x, time_step, axis=1):\n",
    "            output, state = self.rnn_cell(each_time_step, state)\n",
    "            self.output_list.append(output)\n",
    "        self.softmax_weight = tf.Variable(tf.random_normal([self.hidden_size,self.class_num]))\n",
    "        self.softmax_bias = tf.Variable(tf.random_normal([self.class_num]))\n",
    "        logits = tf.matmul(self.output_list[-1],self.softmax_weight) + self.softmax_bias\n",
    "        \n",
    "        self.prediction = tf.nn.softmax(logits)\n",
    "        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=self.label_y))\n",
    "        \n",
    "        correct_pred = tf.equal(tf.argmax(self.prediction,1),tf.argmax(self.label_y,1))\n",
    "        self.accuray = tf.reduce_mean(tf.cast(correct_pred,tf.float32))\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        self.trained = False\n",
    "        \n",
    "    def batch_gen(self,batch_size,polulation_size=500):\n",
    "        collection = []\n",
    "        for _ in ittls.cycle(xrange(polulation_size)):\n",
    "            collection.append(_)\n",
    "            if len(collection) >= batch_size:\n",
    "                yield collection\n",
    "                collection = []\n",
    "#     def mnist_encoder(self,y):\n",
    "#         encoded_y = np.zeros((len(y),10),dtype=np.float32)\n",
    "#         for index,label in enumerate(y):\n",
    "#             encoded_y[index,label] = 1.0\n",
    "#         return encoded_y\n",
    "    def fit(self, sess, x, y, learning_rate = 0.1,iterations=1000):\n",
    "        \"\"\"training process: create optimizer and perform training loop\"\"\"\n",
    "        assert x.shape[1:] == self.input_x.shape[1:]\n",
    "        \n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "        train_op = optimizer.minimize(self.cost)\n",
    "        \n",
    "        index_gen = self.batch_gen(batch_size = self.batch_size, polulation_size=x.shape[0])\n",
    "        \n",
    "        for step in xrange(1,iterations+1):\n",
    "            index = next(index_gen)\n",
    "            feed_x = x[index]\n",
    "            feed_y = y[index]\n",
    "            sess.run(train_op,feed_dict={self.input_x:feed_x, self.label_y:feed_y})\n",
    "            \n",
    "            if step % 200 or step==1:\n",
    "                cost,acc = sess.run([self.cost,self.accuray],\n",
    "                                    feed_dict={self.input_x:feed_x, self.label_y:feed_y})\n",
    "                print \"step:\",step,\",batch_cost=\",\"{:.4f}\".format(cost),\",accuray=\",\"{:.3f}\".format(acc)\n",
    "        self.trained = True\n",
    "        \n",
    "    def predict(sess, x):\n",
    "        return y \n",
    "with tf.Graph().as_default():\n",
    "    with tf.Session() as sess:\n",
    "        x = np.random.random((500,10,15))\n",
    "        y = np.random.binomial(n=1, p=0.4, size=(500,1))\n",
    "        encoded_y = np.zeros((len(y),2),dtype=np.float32)\n",
    "        for index,label in enumerate(y):\n",
    "            encoded_y[index,label] = 1.0\n",
    "        print y.shape\n",
    "        lstm = basic_LSTM(sess=sess, time_step= 10, input_dim = 15, hidden_size = 20, class_num = 2, batch_size = 50)\n",
    "        lstm.fit(sess=sess, x=x, y=encoded_y, learning_rate = 0.001,iterations=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
